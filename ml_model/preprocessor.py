"""
MODULE 3: DATA PREPROCESSING
Comprehensive preprocessing pipeline: cleaning, encoding, scaling, and SMOTE
"""

import os
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import joblib
import config
from ml_model.utils import (
    log_message, print_section_header, format_number, format_time,
    save_figure, write_text_report, Timer
)


def clean_data(df, label_col, use_all_classes=False):
    """
    Clean data by removing useless columns, bad labels, NaN/Inf rows, and duplicates.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Raw dataset
    label_col : str
        Label column name
    use_all_classes : bool
        If True, keep Infilteration class. If False, remove it.
        
    Returns:
    --------
    df_clean : pandas.DataFrame
        Cleaned dataset
    cleaning_stats : dict
        Cleaning statistics
    """
    log_message("Starting data cleaning...", level="STEP")
    
    # Record initial state
    n_rows_initial = len(df)
    n_cols_initial = len(df.columns)
    initial_memory = df.memory_usage(deep=True).sum() / (1024 ** 3)
    
    log_message(f"Initial dataset: {format_number(n_rows_initial)} rows × {n_cols_initial} columns", level="INFO")
    log_message(f"Initial memory: {initial_memory:.2f} GB", level="INFO")
    print()
    
    cleaning_stats = {
        'initial_rows': n_rows_initial,
        'initial_cols': n_cols_initial,
        'initial_memory_gb': initial_memory
    }
    
    # Step 1: Drop useless columns (100% NaN or not needed for ML)
    log_message("Removing useless columns...", level="SUBSTEP")
    
    useless_cols = [col for col in config.PREPROCESSING_DROP_COLUMNS if col in df.columns]
    
    if useless_cols:
        df = df.drop(columns=useless_cols)
        log_message(f"Dropped {len(useless_cols)} useless columns: {', '.join(useless_cols)}", level="INFO")
        cleaning_stats['dropped_columns'] = useless_cols
        cleaning_stats['cols_after_drop'] = len(df.columns)
    else:
        log_message("No useless columns found to drop", level="INFO")
        cleaning_stats['dropped_columns'] = []
        cleaning_stats['cols_after_drop'] = n_cols_initial
    print()
    
    # Step 2: Remove "Label" class (bad data - 59 rows)
    log_message("Removing invalid 'Label' class rows...", level="SUBSTEP")
    
    label_class_mask = df[label_col] == 'Label'
    label_class_count = label_class_mask.sum()
    
    if label_class_count > 0:
        df = df[~label_class_mask].copy()
        log_message(f"Removed {format_number(label_class_count)} rows with class='Label' (bad data)", level="INFO")
        cleaning_stats['label_class_removed'] = label_class_count
    else:
        log_message("No 'Label' class rows found", level="INFO")
        cleaning_stats['label_class_removed'] = 0
    print()
    
    # Step 3: Remove NaN values
    log_message("Removing rows with NaN values...", level="SUBSTEP")
    
    rows_with_nan = df.isnull().any(axis=1).sum()
    nan_cols = df.columns[df.isnull().any()].tolist()
    
    log_message(f"Found {format_number(rows_with_nan)} rows with NaN ({rows_with_nan/len(df)*100:.2f}%)", level="INFO")
    if nan_cols:
        log_message(f"Affected columns: {', '.join(nan_cols[:10])}", level="INFO")
    
    df = df.dropna()
    n_after_nan = len(df)
    nan_removed = n_rows_initial - n_after_nan
    
    log_message(f"Removed {format_number(nan_removed)} rows with NaN", level="SUCCESS")
    log_message(f"Remaining: {format_number(n_after_nan)} rows", level="INFO")
    
    cleaning_stats['nan_rows'] = nan_removed
    cleaning_stats['nan_percentage'] = (nan_removed / n_rows_initial) * 100
    cleaning_stats['nan_cols'] = len(nan_cols)
    cleaning_stats['affected_columns_nan'] = nan_cols
    print()
    
    # Step 4: Remove Infinite values
    log_message("Removing rows with Inf values...", level="SUBSTEP")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    inf_mask = df[numeric_cols].isin([np.inf, -np.inf]).any(axis=1)
    rows_with_inf = inf_mask.sum()
    
    if rows_with_inf > 0:
        inf_cols = []
        for col in numeric_cols:
            if df[col].isin([np.inf, -np.inf]).any():
                inf_cols.append(col)
        
        log_message(f"Found {format_number(rows_with_inf)} rows with Inf ({rows_with_inf/len(df)*100:.2f}%)", level="INFO")
        log_message(f"Affected columns: {', '.join(inf_cols)}", level="INFO")
        
        df = df[~inf_mask].copy()
        n_after_inf = len(df)
        inf_removed = n_after_nan - n_after_inf
        
        log_message(f"Removed {format_number(inf_removed)} rows with Inf", level="SUCCESS")
        log_message(f"Remaining: {format_number(n_after_inf)} rows", level="INFO")
        
        cleaning_stats['inf_rows'] = inf_removed
        cleaning_stats['inf_percentage'] = (inf_removed / n_rows_initial) * 100
        cleaning_stats['inf_cols'] = len(inf_cols)
        cleaning_stats['affected_columns_inf'] = inf_cols
    else:
        log_message("No Inf values found", level="INFO")
        cleaning_stats['inf_rows'] = 0
        cleaning_stats['inf_percentage'] = 0.0
        cleaning_stats['inf_cols'] = 0
        cleaning_stats['affected_columns_inf'] = []
    print()
    
    # Step 5: Remove duplicates
    log_message("Removing duplicate rows...", level="SUBSTEP")
    
    n_before_dup = len(df)
    dup_count = df.duplicated().sum()
    
    log_message(f"Found {format_number(dup_count)} duplicate rows ({dup_count/n_before_dup*100:.2f}%)", level="INFO")
    
    df = df.drop_duplicates()
    n_final = len(df)
    dup_removed = n_before_dup - n_final
    
    log_message(f"Removed {format_number(dup_removed)} duplicate rows", level="SUCCESS")
    log_message(f"Remaining: {format_number(n_final)} rows", level="INFO")
    
    cleaning_stats['duplicate_rows'] = dup_removed
    cleaning_stats['duplicate_percentage'] = (dup_removed / n_rows_initial) * 100
    print()
    
    # Calculate final statistics
    total_removed = n_rows_initial - n_final
    removal_percentage = (total_removed / n_rows_initial) * 100
    final_memory = df.memory_usage(deep=True).sum() / (1024 ** 3)
    memory_saved = initial_memory - final_memory
    
    cleaning_stats['final_rows'] = n_final
    cleaning_stats['final_cols'] = len(df.columns)
    cleaning_stats['total_removed'] = total_removed
    cleaning_stats['removal_percentage'] = removal_percentage
    cleaning_stats['final_memory_gb'] = final_memory
    cleaning_stats['memory_saved_gb'] = memory_saved
    
    # Log summary
    log_message("=" * 80, level="INFO")
    log_message(" " * 28 + "DATA CLEANING SUMMARY", level="SUCCESS")
    log_message("=" * 80, level="INFO")
    log_message(f"Initial rows:       {format_number(n_rows_initial)}", level="INFO")
    log_message(f"Useless columns:    {len(useless_cols)} dropped", level="INFO")
    log_message(f"Invalid 'Label':    {format_number(label_class_count)} rows removed", level="INFO")
    log_message(f"Rows with NaN:      {format_number(nan_removed)} ({cleaning_stats['nan_percentage']:.2f}%)", level="INFO")
    log_message(f"Rows with Inf:      {format_number(cleaning_stats['inf_rows'])} ({cleaning_stats['inf_percentage']:.2f}%)", level="INFO")
    log_message(f"Duplicate rows:     {format_number(dup_removed)} ({cleaning_stats['duplicate_percentage']:.2f}%)", level="INFO")
    log_message("-" * 80, level="INFO")
    log_message(f"Total removed:      {format_number(total_removed)} ({removal_percentage:.2f}%)", level="WARNING" if removal_percentage > 5 else "INFO")
    log_message(f"Final rows:         {format_number(n_final)}", level="SUCCESS")
    log_message(f"Final columns:      {len(df.columns)}", level="INFO")
    log_message("=" * 80, level="INFO")
    log_message(f"Memory before:      {initial_memory:.2f} GB", level="INFO")
    log_message(f"Memory after:       {final_memory:.2f} GB", level="INFO")
    log_message(f"Memory saved:       {memory_saved:.2f} GB", level="SUCCESS")
    log_message("=" * 80, level="INFO")
    
    if removal_percentage > 5.0:
        log_message(f"⚠️  WARNING: Data loss ({removal_percentage:.2f}%) exceeds 5% threshold!", level="WARNING")
        log_message("Continuing anyway as per configuration...", level="WARNING")
    
    print()
    
    # Convert Dst Port to numeric (if exists and is object type)
    if 'Dst Port' in df.columns and df['Dst Port'].dtype == 'object':
        log_message("[SUBSTEP] Converting 'Dst Port' to numeric...", level="SUBSTEP")
        df['Dst Port'] = pd.to_numeric(df['Dst Port'], errors='coerce')
        # Remove any rows where conversion failed
        dst_port_nan = df['Dst Port'].isna().sum()
        if dst_port_nan > 0:
            log_message(f"Removed {dst_port_nan} rows with invalid Dst Port", level="INFO")
            df = df.dropna(subset=['Dst Port'])
        log_message("✓ Dst Port converted to numeric", level="SUCCESS")
        print()
    
    # Keep Protocol as string (will be one-hot encoded later)
    # Convert to string type explicitly to avoid parquet issues
    if 'Protocol' in df.columns and df['Protocol'].dtype == 'object':
        log_message("[SUBSTEP] Standardizing 'Protocol' column type...", level="SUBSTEP")
        df['Protocol'] = df['Protocol'].astype(str)
        log_message("✓ Protocol standardized as string type", level="SUCCESS")
        print()
    
    # Remove rows marked for dropping BEFORE consolidation
    # NOTE: LABEL_MAPPING hasn't been applied yet, so check for RAW label values that map to __DROP__
    drop_mask = pd.Series([False] * len(df), index=df.index)
    drop_values = [k for k, v in config.LABEL_MAPPING.items() if v == '__DROP__']
    
    for drop_val in drop_values:
        drop_mask |= (df[label_col] == drop_val)
    
    n_to_drop = drop_mask.sum()
    
    # Conditionally drop Infilteration rows based on use_all_classes flag
    n_infilteration = 0
    if not use_all_classes:  # Only drop Infilteration if NOT using all classes
        infilteration_mask = df[label_col] == 'Infilteration'
        n_infilteration = infilteration_mask.sum()
        drop_mask = drop_mask | infilteration_mask  # Combine both masks
    
    if n_to_drop > 0:
        log_message(f"Removing {format_number(n_to_drop)} rows marked as '__DROP__' (e.g., SQL Injection, Heartbleed)", level="INFO")
    
    if n_infilteration > 0:
        log_message(f"Removing {format_number(n_infilteration)} rows with attack type 'Infilteration'", level="INFO")
    
    total_to_drop = drop_mask.sum()
    if total_to_drop > 0:
        df = df[~drop_mask].copy()
        if use_all_classes:
            log_message(f"✓ Total removed: {format_number(total_to_drop)} rows ({', '.join(drop_values)})", level="SUCCESS")
        else:
            log_message(f"✓ Total removed: {format_number(total_to_drop)} rows ({', '.join(drop_values)} + Infilteration)", level="SUCCESS")
        log_message(f"Remaining: {format_number(len(df))} rows", level="INFO")
        print()
        cleaning_stats['dropped_rows'] = total_to_drop
    else:
        cleaning_stats['dropped_rows'] = 0
    
    return df, cleaning_stats


def consolidate_labels(df, label_col):
    """
    Merge attack subcategories into parent classes.
    Maps original labels (typically 13-15 variants in raw data) to 6 consolidated classes:
    Benign, Botnet, Brute Force, DDoS, DoS, Infilteration
    (SQL Injection and Heartbleed are marked __DROP__ and removed in clean_data)
    Final count: 5 classes (0-4) if Infilteration removed, 6 classes (0-5) if kept.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Cleaned dataset
    label_col : str
        Label column name
        
    Returns:
    --------
    df : pandas.DataFrame
        Dataset with consolidated labels
    consolidation_stats : dict
        Consolidation statistics
    """
    log_message("Consolidating attack labels...", level="STEP")
    
    # Original labels
    original_labels = df[label_col].value_counts().sort_index()
    n_original = len(original_labels)
    
    log_message(f"Original classes: {n_original}", level="INFO")
    print()
    
    # Apply mapping
    df[label_col] = df[label_col].map(config.LABEL_MAPPING).fillna(df[label_col])
    
    # Note: __DROP__ rows are now removed in clean_data() before checkpoint 1
    # This avoids saving them to disk unnecessarily
    
    # New labels
    consolidated_labels = df[label_col].value_counts().sort_index()
    n_consolidated = len(consolidated_labels)
    
    log_message(f"Consolidated classes: {n_consolidated}", level="SUCCESS")
    log_message(f"Reduction: {n_original} → {n_consolidated} classes", level="INFO")
    print()
    
    # Display mapping
    log_message("Class distribution after consolidation:", level="INFO")
    for label, count in consolidated_labels.items():
        pct = count / len(df) * 100
        log_message(f"  {label:20s}: {format_number(count):>12s} ({pct:>6.2f}%)", level="INFO")
    print()
    
    consolidation_stats = {
        'original_classes': n_original,
        'consolidated_classes': n_consolidated,
        'original_distribution': original_labels.to_dict(),
        'consolidated_distribution': consolidated_labels.to_dict()
    }
    
    return df, consolidation_stats


def encode_features(df, label_col, protocol_col):
    """
    Encode categorical features (one-hot Protocol, label encode target).
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Dataset with consolidated labels
    label_col : str
        Label column name
    protocol_col : str
        Protocol column name (if exists)
        
    Returns:
    --------
    df_encoded : pandas.DataFrame
        Dataset with encoded features
    label_encoder : LabelEncoder
        Fitted label encoder
    encoding_stats : dict
        Encoding statistics
        
    NOTE:
    -----
    Final number of classes depends on variant:
    • Variant 1 (default): 5 classes (0-4) - Infilteration REMOVED after consolidation
    • Variant 2 (all): 6 classes (0-5) - Infilteration KEPT
    
    Classes mapped to __DROP__ (SQL Injection, Heartbleed) are filtered OUT in clean_data()
    BEFORE this function, so they never become actual classes.
    """
    log_message("Encoding categorical features...", level="STEP")
    
    n_cols_before = len(df.columns)
    
    # One-hot encode Protocol (if exists)
    if protocol_col and protocol_col in df.columns:
        log_message(f"One-hot encoding '{protocol_col}' column...", level="SUBSTEP")
        
        protocol_values = df[protocol_col].unique()
        log_message(f"Found {len(protocol_values)} unique protocols: {sorted(protocol_values)}", level="INFO")
        
        df = pd.get_dummies(df, columns=[protocol_col], prefix=protocol_col, drop_first=False)
        
        # Get new column names
        protocol_cols = [col for col in df.columns if col.startswith(protocol_col + '_')]
        log_message(f"Created {len(protocol_cols)} one-hot columns: {', '.join(protocol_cols)}", level="SUCCESS")
        print()
    else:
        log_message("No Protocol column to encode", level="INFO")
        protocol_cols = []
        print()
    
    # Label encode target
    log_message(f"Label encoding target column '{label_col}'...", level="SUBSTEP")
    
    label_encoder = LabelEncoder()
    df[label_col] = label_encoder.fit_transform(df[label_col])
    
    # Create mapping display
    class_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}
    log_message(f"Encoded {len(class_mapping)} classes (actual, not including __DROP__ rows):", level="INFO")
    for idx, label in class_mapping.items():
        log_message(f"  {idx}: {label}", level="INFO")
    print()
    
    n_cols_after = len(df.columns)
    cols_added = n_cols_after - n_cols_before
    
    log_message(f"Encoding complete: {n_cols_before} → {n_cols_after} columns (+{cols_added})", level="SUCCESS")
    print()
    
    encoding_stats = {
        'original_columns': n_cols_before,
        'encoded_columns': n_cols_after,
        'columns_added': cols_added,
        'protocol_columns': protocol_cols,
        'n_classes': len(class_mapping),
        'class_mapping': class_mapping
    }
    
    return df, label_encoder, encoding_stats


def split_data(df, label_col, test_size=0.20, random_state=42):
    """
    Split dataset into train and test sets with stratification.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Encoded dataset
    label_col : str
        Label column name
    test_size : float
        Test set proportion
    random_state : int
        Random seed
        
    Returns:
    --------
    X_train, X_test, y_train, y_test : arrays/DataFrames
        Split datasets
    split_stats : dict
        Split statistics
    """
    log_message("Splitting into train and test sets...", level="STEP")
    
    # Separate features and labels
    feature_cols = [col for col in df.columns if col != label_col]
    X = df[feature_cols]
    y = df[label_col]
    
    log_message(f"Features shape: {X.shape}", level="INFO")
    log_message(f"Labels shape: {y.shape}", level="INFO")
    print()
    
    # Class distribution before split
    log_message("Class distribution before split:", level="INFO")
    class_counts = y.value_counts().sort_index()
    for class_idx, count in class_counts.items():
        pct = count / len(y) * 100
        log_message(f"  Class {class_idx}: {format_number(count):>10s} ({pct:>6.2f}%)", level="INFO")
    print()
    
    # Stratified split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        stratify=y,
        random_state=random_state
    )
    
    log_message(f"Train set: {format_number(len(X_train))} samples ({(1-test_size)*100:.0f}%)", level="SUCCESS")
    log_message(f"Test set:  {format_number(len(X_test))} samples ({test_size*100:.0f}%)", level="SUCCESS")
    print()
    
    # Verify stratification
    log_message("Verifying stratification:", level="SUBSTEP")
    train_dist = y_train.value_counts(normalize=True).sort_index()
    test_dist = y_test.value_counts(normalize=True).sort_index()
    
    max_diff = 0
    for class_idx in train_dist.index:
        diff = abs(train_dist[class_idx] - test_dist[class_idx])
        max_diff = max(max_diff, diff)
    
    log_message(f"Max distribution difference: {max_diff*100:.3f}%", level="SUCCESS" if max_diff < 0.01 else "WARNING")
    log_message("✓ Stratification verified - train and test have same class proportions", level="SUCCESS")
    print()
    
    split_stats = {
        'total_samples': len(df),
        'n_features': X.shape[1],
        'n_train': len(X_train),
        'n_test': len(X_test),
        'train_percentage': (1 - test_size) * 100,
        'test_percentage': test_size * 100,
        'test_size': test_size,
        'random_state': random_state,
        'stratified': True,
        'max_distribution_diff': max_diff
    }
    
    return X_train, X_test, y_train, y_test, split_stats


def scale_features(X_train, X_test, scaler_type='standard'):
    """
    Scale features using StandardScaler fitted on training data only.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        Training features
    X_test : pandas.DataFrame
        Test features
    scaler_type : str
        'standard' or 'minmax'
        
    Returns:
    --------
    X_train_scaled, X_test_scaled : pandas.DataFrame
        Scaled features
    scaler : fitted scaler object
    scaling_stats : dict
        Scaling statistics
    """
    log_message("Scaling features...", level="STEP")
    
    # Select scaler
    if scaler_type == 'standard':
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        log_message("Using StandardScaler (mean=0, std=1)", level="INFO")
    else:
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        log_message("Using MinMaxScaler (range [0, 1])", level="INFO")
    print()
    
    # Fit on training data ONLY
    log_message("Fitting scaler on TRAINING data only...", level="SUBSTEP")
    scaler.fit(X_train)
    log_message("✓ Scaler fitted (no data leakage)", level="SUCCESS")
    print()
    
    # Transform training data
    log_message("Transforming training data...", level="SUBSTEP")
    X_train_scaled = scaler.transform(X_train)
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    log_message(f"Training data scaled: {X_train_scaled.shape}", level="SUCCESS")
    print()
    
    # Transform test data (using training statistics)
    log_message("Transforming test data using TRAINING statistics...", level="SUBSTEP")
    X_test_scaled = scaler.transform(X_test)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
    log_message(f"Test data scaled: {X_test_scaled.shape}", level="SUCCESS")
    log_message("✓ No data leakage - test data did not influence scaler", level="SUCCESS")
    print()
    
    # Verify scaling on training data
    train_means = X_train_scaled.mean()
    train_stds = X_train_scaled.std()
    
    if scaler_type == 'standard':
        log_message("Verification (training set should have mean≈0, std≈1):", level="INFO")
        log_message(f"  Mean range: [{train_means.min():.6f}, {train_means.max():.6f}]", level="INFO")
        log_message(f"  Std range:  [{train_stds.min():.6f}, {train_stds.max():.6f}]", level="INFO")
    
    print()
    
    scaling_stats = {
        'scaler_type': scaler_type,
        'n_features': X_train.shape[1],
        'train_shape': X_train_scaled.shape,
        'test_shape': X_test_scaled.shape
    }
    
    return X_train_scaled, X_test_scaled, scaler, scaling_stats


def apply_smote(X_train, y_train, target_percentage=0.03, k_neighbors=5, random_state=42, strategy='dynamic', tiered_targets=None):
    """
    Apply SMOTE to training data only to balance classes.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        Training features (scaled)
    y_train : pandas.Series
        Training labels
    target_percentage : float
        Target percentage for uniform strategy (ignored if strategy='dynamic')
    k_neighbors : int
        Number of neighbors for SMOTE
    random_state : int
        Random seed
    strategy : str
        'uniform' (all minorities to same %), 'tiered' (config-based targets), 
        or 'dynamic' (relative to 2nd largest class - RECOMMENDED)
    tiered_targets : dict
        {class_index: target_percentage} for tiered strategy
        
    Returns:
    --------
    X_train_smoted, y_train_smoted : arrays/DataFrames
        Resampled training data
    smote_stats : dict
        SMOTE statistics
    """
    log_message("Applying SMOTE to balance classes...", level="STEP")
    log_message("⚠️  SMOTE applied ONLY to training data (test remains imbalanced)", level="WARNING")
    print()
    
    # Class distribution before SMOTE
    class_counts_before = y_train.value_counts().sort_index()
    total_samples = len(y_train)
    
    log_message("Class distribution BEFORE SMOTE:", level="INFO")
    for class_idx, count in class_counts_before.items():
        pct = count / total_samples * 100
        log_message(f"  Class {class_idx}: {format_number(count):>10s} ({pct:>6.2f}%)", level="INFO")
    print()
    
    # Calculate target counts based on strategy
    sampling_strategy = {}
    
    if strategy == 'dynamic':
        # DYNAMIC STRATEGY: Use 2nd largest class as reference
        log_message(f"Using DYNAMIC sampling strategy (relative to 2nd largest class)", level="INFO")
        
        # Sort classes by count (descending)
        sorted_counts = class_counts_before.sort_values(ascending=False)
        
        if len(sorted_counts) >= 2:
            # Get 2nd largest class count as reference
            second_largest_count = sorted_counts.iloc[1]
            log_message(f"Reference (2nd largest class): {format_number(second_largest_count)} samples", level="INFO")
            print()
            
            # For each class smaller than 2nd largest, calculate target dynamically
            for class_idx, current_count in class_counts_before.items():
                if current_count < second_largest_count:
                    # Target = current + (2nd_largest - current) / 2
                    # This brings each class halfway to the 2nd largest
                    target_count = int(current_count + (second_largest_count - current_count) / 2)
                    sampling_strategy[class_idx] = target_count
                    pct_increase = ((target_count - current_count) / current_count * 100) if current_count > 0 else 0
                    log_message(f"  Class {class_idx}: {format_number(current_count)} → {format_number(target_count)} (+{pct_increase:.1f}%)", level="INFO")
        else:
            log_message("⚠️  Only 1 class found, no SMOTE needed", level="WARNING")
    
    elif strategy == 'tiered' and tiered_targets:
        log_message(f"Using TIERED sampling strategy (config-based percentages)", level="INFO")
        for class_idx, target_pct in tiered_targets.items():
            current_count = class_counts_before.get(class_idx, 0)
            target_count = int(total_samples * target_pct)
            if current_count < target_count:
                sampling_strategy[class_idx] = target_count
                log_message(f"  Class {class_idx}: {format_number(current_count)} → {format_number(target_count)} (target: {target_pct*100:.1f}%)", level="INFO")
    else:
        log_message(f"Using UNIFORM sampling strategy (target: {target_percentage*100:.1f}%)", level="INFO")
        target_count = int(total_samples * target_percentage)
        for class_idx, count in class_counts_before.items():
            if count < target_count:
                sampling_strategy[class_idx] = target_count
    
    log_message(f"Classes to oversample: {len(sampling_strategy)}", level="INFO")
    print()
    
    # Apply SMOTE
    log_message("Running SMOTE (this may take 15-20 minutes)...", level="SUBSTEP")
    
    smote = SMOTE(
        sampling_strategy=sampling_strategy,
        k_neighbors=k_neighbors,
        random_state=random_state
    )
    
    timer = Timer("SMOTE", verbose=False)
    timer.__enter__()
    
    X_train_smoted, y_train_smoted = smote.fit_resample(X_train, y_train)
    
    timer.__exit__()
    
    # Convert back to DataFrame
    X_train_smoted = pd.DataFrame(X_train_smoted, columns=X_train.columns)
    y_train_smoted = pd.Series(y_train_smoted, name=y_train.name)
    
    # Class distribution after SMOTE
    class_counts_after = y_train_smoted.value_counts().sort_index()
    
    log_message("Class distribution AFTER SMOTE:", level="SUCCESS")
    for class_idx, count in class_counts_after.items():
        pct = count / len(y_train_smoted) * 100
        before_count = class_counts_before.get(class_idx, 0)
        increase = count - before_count
        log_message(f"  Class {class_idx}: {format_number(count):>10s} ({pct:>6.2f}%) [+{format_number(increase)}]", level="INFO")
    print()
    
    log_message(f"Training samples: {format_number(len(y_train))} → {format_number(len(y_train_smoted))}", level="SUCCESS")
    log_message(f"Increase: {format_number(len(y_train_smoted) - len(y_train))} synthetic samples", level="INFO")
    print()
    
    smote_stats = {
        'before_count': len(y_train),
        'after_count': len(y_train_smoted),
        'synthetic_samples': len(y_train_smoted) - len(y_train),
        'target_percentage': target_percentage,
        'k_neighbors': k_neighbors,
        'classes_oversampled': len(sampling_strategy),
        'distribution_before': class_counts_before.to_dict(),
        'distribution_after': class_counts_after.to_dict(),
        'strategy': strategy
    }
    
    return X_train_smoted, y_train_smoted, smote_stats


def eliminate_highly_correlated_features_from_exploration(X_train, corr_threshold=0.99):
    """
    Eliminate highly correlated features using precomputed correlation data from exploration.
    This avoids recomputing the correlation matrix which is expensive.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        Training features
    corr_threshold : float
        Correlation threshold (default 0.99 for perfect/near-perfect correlations)
        
    Returns:
    --------
    X_train_reduced : pandas.DataFrame
        Training data with correlated features removed
    features_removed : list
        Features removed due to correlation
    features_kept : list
        Features retained
    correlation_stats : dict
        Statistics about removed features
    """
    log_message("Eliminating highly correlated features using exploration data (|r| >= {})...".format(corr_threshold), level="STEP")
    print()
    
    n_features_before = X_train.shape[1]
    log_message(f"Initial features: {n_features_before}", level="INFO")
    
    # Try to load precomputed correlation data from exploration
    corr_file = config.EXPLORATION_CORRELATION_FILE
    
    if os.path.exists(corr_file):
        log_message("Loading precomputed correlation data from exploration...", level="SUBSTEP")
        corr_data = joblib.load(corr_file)
        corr_matrix = corr_data['correlation_matrix']
        highly_corr_pairs = corr_data['highly_correlated_pairs_all']
        log_message(f"✓ Loaded precomputed correlation matrix ({corr_matrix.shape[0]} × {corr_matrix.shape[1]} features)", level="SUCCESS")
    else:
        log_message("Correlation data not found from exploration - computing locally...", level="WARNING")
        # Fallback: compute locally
        log_message("Computing correlation matrix...", level="SUBSTEP")
        corr_matrix = X_train.corr().abs()
        highly_corr_pairs = []
        
        # Find pairs with correlation >= threshold
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        for column in upper_tri.columns:
            high_corr_features = upper_tri[upper_tri[column] >= corr_threshold].index.tolist()
            for feature in high_corr_features:
                corr_val = corr_matrix.loc[feature, column]
                highly_corr_pairs.append((feature, column, corr_val))
    
    print()
    
    # Filter pairs that are actually present in current X_train
    available_pairs = [
        (feat1, feat2, corr_val) 
        for feat1, feat2, corr_val in highly_corr_pairs
        if feat1 in X_train.columns and feat2 in X_train.columns and abs(corr_val) >= corr_threshold
    ]
    
    # Determine which features to drop (keep first, drop second of each pair)
    to_drop = set()
    for feat1, feat2, corr_val in available_pairs:
        if feat1 not in to_drop and feat2 not in to_drop:
            to_drop.add(feat2)  # Drop second feature
    
    features_to_drop = sorted(list(to_drop))
    features_kept = [f for f in X_train.columns if f not in features_to_drop]
    
    log_message(f"Found {len(available_pairs)} highly correlated pairs (|r| >= {corr_threshold})", level="INFO")
    log_message(f"Features to remove: {len(features_to_drop)}", level="WARNING")
    
    if len(features_to_drop) > 0:
        log_message("Removed features:", level="INFO")
        for feat in features_to_drop[:15]:  # Show first 15
            log_message(f"  - {feat}", level="INFO")
        if len(features_to_drop) > 15:
            log_message(f"  ... and {len(features_to_drop) - 15} more", level="INFO")
    
    # Remove correlated features
    X_train_reduced = X_train[features_kept].copy()
    
    log_message(f"✓ Correlation-based elimination complete", level="SUCCESS")
    log_message(f"  Original features: {n_features_before}", level="INFO")
    log_message(f"  Features after elimination: {len(features_kept)}", level="INFO")
    log_message(f"  Reduction: {(1 - len(features_kept)/n_features_before)*100:.1f}%", level="INFO")
    print()
    
    correlation_stats = {
        'n_features_before': n_features_before,
        'n_features_after': len(features_kept),
        'n_features_removed': len(features_to_drop),
        'n_pairs_found': len(available_pairs),
        'corr_threshold': corr_threshold,
        'removed_features': features_to_drop,
        'kept_features': features_kept,
        'correlated_pairs': available_pairs,
        'used_precomputed_data': os.path.exists(corr_file)
    }
    
    return X_train_reduced, features_to_drop, features_kept, correlation_stats


def eliminate_highly_correlated_features(X_train, corr_threshold=0.99):
    """
    Eliminate highly correlated features by keeping only one from each correlated pair.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        Training features
    corr_threshold : float
        Correlation threshold (default 0.99 for perfect/near-perfect correlations)
        
    Returns:
    --------
    X_train_reduced : pandas.DataFrame
        Training data with correlated features removed
    features_removed : list
        Features removed due to correlation
    features_kept : list
        Features retained
    correlation_stats : dict
        Statistics about removed features
    """
    # Delegate to the new function that uses exploration data
    return eliminate_highly_correlated_features_from_exploration(X_train, corr_threshold)


def perform_rf_feature_importance(X_train, y_train, min_features=40, max_features=45, random_state=42, output_dir=None, trained_model_dir=None):
    """
    Perform feature selection using Random Forest Gini importance method.
    Fast and effective - typical runtime 8-12 minutes depending on dataset size.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        Training features (scaled + SMOTEd)
    y_train : pandas.Series
        Training labels (SMOTEd)
    min_features : int
        Minimum features to keep
    max_features : int
        Maximum features to keep
    random_state : int
        Random seed
    output_dir : str, optional
        Directory to save feature importances. If None, uses DATA_PREPROCESSED_DIR
    trained_model_dir : str, optional
        Deprecated - no longer used. Trainer.py will copy necessary files.
    
    Returns:
    --------
    X_train_selected : pandas.DataFrame
        Training data with selected features
    selected_features : list
        Selected feature names
    rf_model : fitted RandomForest object
    importance_stats : dict
        Feature importance statistics
    """
    log_message("Performing RF Feature Importance Selection (Random Forest Gini Importance)...", level="STEP")
    log_message("⏱️  Expected time: 8-12 minutes (depends on dataset size)", level="INFO")
    print()
    
    n_features_before = X_train.shape[1]
    log_message(f"Initial features: {n_features_before}", level="INFO")
    log_message(f"Target range: {min_features}-{max_features} features", level="INFO")
    print()
    
    # Use subset for speed if dataset is large
    use_subset = len(X_train) > config.RF_IMPORTANCE_SUBSET_THRESHOLD
    if use_subset:
        subset_size = config.RF_IMPORTANCE_SUBSET_SIZE
        log_message(f"Dataset is large ({len(X_train):,} samples)", level="WARNING")
        log_message(f"Using stratified subset of {subset_size:,} samples for speed", level="INFO")
        
        from sklearn.model_selection import train_test_split
        X_subset, _, y_subset, _ = train_test_split(
            X_train, y_train, 
            train_size=subset_size,
            stratify=y_train,
            random_state=random_state
        )
        log_message(f"✓ Subset created: {X_subset.shape[0]:,} samples", level="SUCCESS")
    else:
        X_subset = X_train
        y_subset = y_train
    
    # Train Random Forest for importance
    log_message("Training Random Forest for feature importance...", level="SUBSTEP")
    start_time = time.time()
    
    rf_model = RandomForestClassifier(
        n_estimators=config.RF_IMPORTANCE_TREES,
        max_depth=config.RF_IMPORTANCE_MAX_DEPTH,
        n_jobs=config.N_JOBS,
        random_state=random_state,
        verbose=1
    )
    
    log_message(f"RF config: {config.RF_IMPORTANCE_TREES} trees, depth={config.RF_IMPORTANCE_MAX_DEPTH}, n_jobs={config.N_JOBS}", level="INFO")
    rf_model.fit(X_subset, y_subset)
    elapsed = time.time() - start_time
    log_message(f"✓ RF training complete in {elapsed/60:.2f} minutes", level="SUCCESS")
    print()
    
    # Get feature importances
    log_message("Extracting feature importances...", level="SUBSTEP")
    importances = rf_model.feature_importances_
    feature_names = X_train.columns.tolist()
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    log_message("Top 10 Most Important Features:", level="INFO")
    for idx, row in importance_df.head(10).iterrows():
        log_message(f"  {row['feature']:<40} {row['importance']:.6f}", level="INFO")
    print()
    
    # Select features based on threshold and target range
    threshold = config.IMPORTANCE_THRESHOLD
    features_above_threshold = importance_df[importance_df['importance'] >= threshold]
    
    if len(features_above_threshold) < min_features:
        log_message(f"Only {len(features_above_threshold)} features above threshold - selecting top {min_features}", level="WARNING")
        selected_features_df = importance_df.head(min_features)
    elif len(features_above_threshold) > max_features:
        log_message(f"{len(features_above_threshold)} features above threshold - limiting to top {max_features}", level="INFO")
        selected_features_df = importance_df.head(max_features)
    else:
        log_message(f"Selecting {len(features_above_threshold)} features above threshold {threshold}", level="INFO")
        selected_features_df = features_above_threshold
    
    selected_features = selected_features_df['feature'].tolist()
    
    log_message("Feature Selection Summary:", level="INFO")
    log_message(f"  Original features: {n_features_before}", level="INFO")
    log_message(f"  Selected features: {len(selected_features)}", level="INFO")
    log_message(f"  Reduction: {(1 - len(selected_features)/n_features_before)*100:.1f}%", level="INFO")
    print()
    
    # Save feature importance report
    if output_dir is None:
        output_dir = config.DATA_PREPROCESSED_DIR
    importance_file = os.path.join(output_dir, 'feature_importances.csv')
    importance_df.to_csv(importance_file, index=False)
    log_message(f"✓ Feature importances saved", level="SUCCESS")
    print()
    
    # Select features from training data
    X_train_selected = X_train[selected_features]
    
    total_time = time.time() - start_time
    log_message(f"✓ Feature selection complete in {total_time/60:.2f} minutes", level="SUCCESS")
    print()
    
    importance_stats = {
        'n_features_before': n_features_before,
        'n_features_selected': len(selected_features),
        'selected_features': selected_features,
        'importance_threshold': threshold,
        'time_minutes': total_time / 60,
        'applied': True,  # FIXED: Added for report generation
        'reduction_percentage': (1 - len(selected_features)/n_features_before)*100,  # FIXED: Added
        'best_score': 0.0,  # RF importance doesn't have CV score, using placeholder
        'cv_folds': 1,  # FIXED: Added for report consistency
        'use_subset': use_subset,  # Track if subset was used
        'n_train_samples': len(X_train),  # Total training samples available
        'features_above_threshold': len([f for f in selected_features]) if threshold > 0 else 0  # Placeholder
    }
    
    return X_train_selected, selected_features, rf_model, importance_stats



def preprocess_data(df, label_col, protocol_col=None, resume_from=None, use_all_classes=False):
    """
    Main preprocessing pipeline with checkpoint resume support.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Raw dataset (None if resuming from checkpoint)
    label_col : str
        Label column name
    protocol_col : str, optional
        Protocol column name
    resume_from : int, optional
        Checkpoint to resume from (1=cleaned, 2=encoded, 3=smoted)
    use_all_classes : bool, optional
        If True, keep Infilteration class (6 classes total)
        If False, remove Infilteration rows (5 classes total)
        
    Returns:
    --------
    dict : All preprocessed datasets and artifacts
    """
    print()
    print_section_header("MODULE 3: DATA PREPROCESSING")
    print()
    
    overall_timer = Timer("Module 3: Data Preprocessing", verbose=False)
    overall_timer.__enter__()
    
    # Update config based on variant
    variant_config = config.get_paths(use_all_classes=use_all_classes)
    output_dir = variant_config['data_preprocessed']
    os.makedirs(output_dir, exist_ok=True)
    
    # Also get trained_model directory for saving preprocessing artifacts
    trained_model_dir = variant_config['trained_model']
    os.makedirs(trained_model_dir, exist_ok=True)
    
    reports_dir = variant_config['reports_preprocessing']
    os.makedirs(reports_dir, exist_ok=True)
    
    # Log variant being used
    variant_name = "6-CLASS (keeping Infilteration)" if use_all_classes else "5-CLASS (removing Infilteration)"
    log_message(f"Preprocessing variant: {variant_name}", level="INFO")
    print()
    
    all_stats = {}
    
    # =========================================================================
    # CHECKPOINT RESUME LOGIC
    # =========================================================================
    if resume_from is not None:
        log_message(f"RESUME MODE: Intermediate checkpoints not supported anymore", level="WARNING")
        log_message(f"Only final train_final.parquet/test_final.parquet are saved", level="INFO")
        raise ValueError("Intermediate checkpoint resume removed. Please run full preprocessing pipeline.")
    
    # =========================================================================
    # NORMAL EXECUTION (No Resume)
    # =========================================================================
    else:
        log_message("Starting fresh preprocessing pipeline", level="INFO")
        print()
    
    try:
        # ========================================================================
        # STEPS 1-3: Clean, Consolidate, Encode (Common for both train/test)
        # ========================================================================
        if resume_from is None:
            # Step 1: Clean data (remove duplicates, NaN, Inf, __DROP__ rows, and conditionally Infilteration)
            df_clean, cleaning_stats = clean_data(df, label_col, use_all_classes=use_all_classes)
            all_stats['cleaning'] = cleaning_stats
            print()
            
            # Step 2: Consolidate labels
            df_consolidated, consolidation_stats = consolidate_labels(df_clean, label_col)
            all_stats['consolidation'] = consolidation_stats
            print()
            
            # Step 3: Encode features
            df_encoded, label_encoder, encoding_stats = encode_features(df_consolidated, label_col, protocol_col)
            all_stats['encoding'] = encoding_stats
            print()
            
            # Step 4: Split data into train/test
            X_train, X_test, y_train, y_test, split_stats = split_data(
                df_encoded, label_col,
                test_size=config.TEST_SIZE,
                random_state=config.RANDOM_STATE
            )
            all_stats['split'] = split_stats
            print()
            
            # ====================================================================
            # STEP 5: Scale features (common for both train and test)
            # ====================================================================
            X_train_scaled, X_test_scaled, scaler, scaling_stats = scale_features(
                X_train, X_test,
                scaler_type=config.SCALER_TYPE
            )
            all_stats['scaling'] = scaling_stats
            
            # Save scaler and label encoder to preprocessed directory
            # (trainer.py will copy these to trained_model)
            scaler_path = os.path.join(output_dir, 'scaler.joblib')
            joblib.dump(scaler, scaler_path)
            
            label_encoder_path = os.path.join(output_dir, 'label_encoder.joblib')
            joblib.dump(label_encoder, label_encoder_path)
            
            log_message(f"✓ Scaler saved: scaler.joblib", level="SUCCESS")
            log_message(f"✓ Label encoder saved: label_encoder.joblib", level="SUCCESS")
            print()
            
            # Test data will be saved with selected features in final step
            # (skipping intermediate checkpoint save)
            
            # ====================================================================
            # STEP 6: Apply SMOTE (training data only)
            # ====================================================================
            if config.APPLY_SMOTE:
                smote_strategy = getattr(config, 'SMOTE_STRATEGY', 'uniform')
                tiered_targets = getattr(config, 'SMOTE_TIERED_TARGETS', None)
                
                # Filter tiered_targets to only include classes that exist in current variant
                if tiered_targets:
                    actual_classes = set(y_train.unique())
                    tiered_targets = {k: v for k, v in tiered_targets.items() if k in actual_classes}
                
                X_train_smoted, y_train_smoted, smote_stats = apply_smote(
                    X_train_scaled, y_train,
                    target_percentage=config.SMOTE_TARGET_PERCENTAGE,
                    k_neighbors=config.SMOTE_K_NEIGHBORS,
                    random_state=config.RANDOM_STATE,
                    strategy=smote_strategy,
                    tiered_targets=tiered_targets
                )
                all_stats['smote'] = smote_stats
            else:
                log_message("SMOTE disabled, skipping...", level="WARNING")
                X_train_smoted = X_train_scaled
                y_train_smoted = y_train
                all_stats['smote'] = {'applied': False}
            print()
            
            # Train data will be saved with selected features in final step
            # (skipping intermediate checkpoint save)
        
        # ====================================================================
        # STEP 7.5: Eliminate highly correlated features (before feature importance)
        # ====================================================================
        log_message("Step 7.5: Correlation-based feature elimination...", level="STEP")
        print()
        
        X_train_corr_reduced, features_corr_removed, features_corr_kept, corr_elim_stats = eliminate_highly_correlated_features(
            X_train_smoted,
            corr_threshold=config.CORR_ELIMINATION_THRESHOLD  # Configurable threshold from config.py
        )
        all_stats['correlation_elimination'] = corr_elim_stats
        
        # Apply same features to test
        X_test_corr_reduced = X_test_scaled[features_corr_kept]
        
        print()
        
        # ====================================================================
        # STEP 8: Feature selection (RF Importance)
        # ====================================================================
        if config.ENABLE_RF_IMPORTANCE:
            X_train_selected, selected_features, rf_model, importance_stats = perform_rf_feature_importance(
                X_train_corr_reduced, y_train_smoted,
                min_features=config.TARGET_FEATURES_MIN,
                max_features=config.TARGET_FEATURES_MAX,
                random_state=config.RANDOM_STATE,
                output_dir=output_dir,
                trained_model_dir=trained_model_dir
            )
            all_stats['rfe'] = importance_stats  # FIXED: Changed from 'feature_importance' to 'rfe'
            
            # Apply same feature selection to test
            X_test_selected = X_test_corr_reduced[selected_features]
            
            X_train_final = X_train_selected
            X_test_final = X_test_selected
        else:
            log_message("Feature selection disabled", level="WARNING")
            all_stats['feature_selection'] = {'applied': False}
            X_train_final = X_train_corr_reduced
            X_test_final = X_test_corr_reduced
            selected_features = X_train_corr_reduced.columns.tolist()
        print()
        
        # ====================================================================
        # STEP 9: Save final features and models
        # ====================================================================
        train_final_path = os.path.join(output_dir, 'train_final.parquet')
        test_final_path = os.path.join(output_dir, 'test_final.parquet')
        pd.concat([X_train_final, y_train_smoted], axis=1).to_parquet(train_final_path, index=False)
        pd.concat([X_test_final, y_test], axis=1).to_parquet(test_final_path, index=False)
        log_message(f"✓ Final data saved: train_final.parquet, test_final.parquet", level="SUCCESS")
        
        # Save feature selection model and features list to preprocessed directory only
        # (selected_features.joblib will be copied to trained_model during training)
        if config.ENABLE_RF_IMPORTANCE:
            rf_path = os.path.join(output_dir, 'rf_importance_model.joblib')
            joblib.dump(rf_model, rf_path)
            log_message(f"✓ RF importance model saved: rf_importance_model.joblib", level="SUCCESS")
        
        features_path = os.path.join(output_dir, 'selected_features.joblib')
        joblib.dump(selected_features, features_path)
        log_message(f"✓ Selected features saved: selected_features.joblib", level="SUCCESS")
        print()
        
        # Generate visualizations
        log_message("Generating preprocessing visualizations...", level="STEP")
        generate_preprocessing_visualizations(all_stats, reports_dir)
        print()
        
        # Generate reports
        log_message("Generating preprocessing reports...", level="STEP")
        generate_preprocessing_report(all_stats, reports_dir)
        generate_preprocessing_steps_log(all_stats, reports_dir)
        print()
        
        overall_timer.__exit__()
        
        log_message("Module 3 completed successfully!", level="SUCCESS")
        log_message(f"All preprocessed data saved to: {output_dir}", level="SUCCESS")
        log_message(f"All reports saved to: {reports_dir}", level="SUCCESS")
        print()
        
        return {
            'X_train': X_train_final,
            'X_test': X_test_final,
            'y_train': y_train_smoted,
            'y_test': y_test,
            'scaler': scaler,
            'label_encoder': label_encoder,
            'stats': all_stats
        }
    
    except Exception as e:
        log_message(f"Module 3 failed: {str(e)}", level="ERROR")
        raise


def generate_preprocessing_visualizations(all_stats, output_dir):
    """
    Generate all preprocessing visualization plots.
    
    Parameters:
    -----------
    all_stats : dict
        All preprocessing statistics
    output_dir : str
        Output directory for plots
    """
    log_message("Generating preprocessing visualizations...", level="STEP")
    print()
    
    # 1. Data Cleaning Summary
    if 'cleaning' in all_stats:
        plot_cleaning_summary(all_stats['cleaning'], output_dir)
    
    # 2. Class Distribution Before SMOTE
    if 'smote' in all_stats and all_stats['smote'].get('applied', True):
        plot_class_distribution_before_smote(all_stats['smote'], output_dir)
        
        # 3. Class Distribution After SMOTE
        plot_class_distribution_after_smote(all_stats['smote'], output_dir)
        
        # 4. SMOTE Comparison (side-by-side)
        plot_smote_comparison(all_stats['smote'], output_dir)
    
    # 5. Feature Importance (if RFE was applied)
    if 'rfe' in all_stats and all_stats['rfe'].get('applied', True):
        plot_rfe_feature_importance(all_stats['rfe'], output_dir)
        plot_rfe_performance_curve(all_stats['rfe'], output_dir)
    
    print()


def plot_cleaning_summary(cleaning_stats, output_dir):
    """Plot data cleaning summary showing rows removed at each step."""
    fig, ax = plt.subplots(figsize=(12, 6))
    
    # Data for waterfall chart
    initial = cleaning_stats.get('initial_rows', 0)
    bad_label = cleaning_stats.get('label_class_removed', 0)
    nan_rows = cleaning_stats.get('nan_rows', 0)
    inf_rows = cleaning_stats.get('inf_rows', 0)
    dup_rows = cleaning_stats.get('duplicate_rows', 0)
    
    steps = ['Initial', 'After NaN\nRemoval', 'After Inf\nRemoval', 'After Duplicate\nRemoval']
    values = [
        initial - bad_label,  # After removing bad 'Label' rows (done before NaN)
        initial - bad_label - nan_rows,
        initial - bad_label - nan_rows - inf_rows,
        cleaning_stats.get('final_rows', 0)
    ]
    
    colors = ['#3498db', '#2ecc71', '#2ecc71', '#27ae60']
    
    bars = ax.bar(steps, values, color=colors, edgecolor='black', linewidth=1.5)
    
    # Add value labels on bars
    initial_for_pct = initial - bad_label
    for i, (bar, val) in enumerate(zip(bars, values)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:,}\n({val/initial_for_pct*100:.1f}%)',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # Add removal annotations
    removed_texts = [
        f'-{nan_rows:,}\nNaN rows',
        f'-{inf_rows:,}\nInf rows',
        f'-{dup_rows:,}\nDuplicates',
        ''
    ]
    
    for i, (bar, text) in enumerate(zip(bars, removed_texts)):
        if text:
            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 0.5,
                   text, ha='center', va='center', fontsize=9, 
                   color='white', fontweight='bold',
                   bbox=dict(boxstyle='round', facecolor='red', alpha=0.7))
    
    ax.set_ylabel('Number of Rows', fontsize=12, fontweight='bold')
    ax.set_title('Data Cleaning Process: Rows at Each Stage', fontsize=14, fontweight='bold', pad=20)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, (initial - bad_label) * 1.15)
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'cleaning_summary.png'))


def plot_class_distribution_before_smote(smote_stats, output_dir):
    """Plot class distribution before SMOTE - both log and linear scales."""
    dist_before = smote_stats.get('distribution_before', {})
    classes = sorted(dist_before.keys())
    counts = [dist_before[c] for c in classes]
    total = sum(counts)
    percentages = [c/total*100 for c in counts]
    
    # Plot 1: Log scale (shows extreme imbalance better)
    fig, ax = plt.subplots(figsize=(10, 8))
    
    colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(classes)))
    bars = ax.bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)
    
    for i, (bar, count, pct) in enumerate(zip(bars, counts, percentages)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height * 1.05,
                f'{count:,}\n({pct:.2f}%)',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax.set_ylabel('Number of Samples (Log Scale)', fontsize=12, fontweight='bold')
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_title('Class Distribution BEFORE SMOTE - Log Scale\n(Shows Extreme Imbalance)', 
                 fontsize=14, fontweight='bold', pad=20)
    ax.set_yscale('log')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    max_count = max(counts)
    min_count = min(counts)
    imbalance_ratio = max_count / min_count
    ax.text(0.98, 0.98, f'Imbalance Ratio: {imbalance_ratio:.0f}:1\n(Severely Imbalanced)',
            transform=ax.transAxes, ha='right', va='top',
            fontsize=10, bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.8))
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'class_distribution_before_smote_log.png'))
    
    # Plot 2: Linear scale (better for comparison)
    fig, ax = plt.subplots(figsize=(10, 8))
    
    bars = ax.bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)
    
    for i, (bar, count, pct) in enumerate(zip(bars, counts, percentages)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,
                f'{count:,}\n({pct:.2f}%)',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax.set_ylabel('Number of Samples (Linear Scale)', fontsize=12, fontweight='bold')
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_title('Class Distribution BEFORE SMOTE - Linear Scale\n(Shows Dominance of Majority Class)', 
                 fontsize=14, fontweight='bold', pad=20)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, max(counts) * 1.15)
    
    ax.text(0.98, 0.98, f'Majority: {max(counts):,} samples\nMinority: {min(counts):,} samples',
            transform=ax.transAxes, ha='right', va='top',
            fontsize=10, bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.8))
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'class_distribution_before_smote_linear.png'))


def plot_class_distribution_after_smote(smote_stats, output_dir):
    """Plot class distribution after SMOTE - both log and linear scales."""
    dist_after = smote_stats.get('distribution_after', {})
    dist_before = smote_stats.get('distribution_before', {})
    classes = sorted(dist_after.keys())
    counts = [dist_after[c] for c in classes]
    total = sum(counts)
    percentages = [c/total*100 for c in counts]
    
    # Plot 1: Log scale
    fig, ax = plt.subplots(figsize=(10, 8))
    
    colors = plt.cm.Greens(np.linspace(0.3, 0.9, len(classes)))
    bars = ax.bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)
    
    for i, (cls, bar, count, pct) in enumerate(zip(classes, bars, counts, percentages)):
        height = bar.get_height()
        before_count = dist_before.get(cls, count)
        increase = count - before_count
        factor = count / before_count if before_count > 0 else 0
        
        if increase > 0:
            label = f'{count:,}\n({pct:.2f}%)\n[+{increase:,}, {factor:.1f}x]'
            color = 'darkgreen'
        else:
            label = f'{count:,}\n({pct:.2f}%)'
            color = 'black'
        
        ax.text(bar.get_x() + bar.get_width()/2., height * 1.05,
                label, ha='center', va='bottom', fontsize=8, fontweight='bold', color=color)
    
    ax.set_ylabel('Number of Samples (Log Scale)', fontsize=12, fontweight='bold')
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_title('Class Distribution AFTER SMOTE - Log Scale\n(Shows All Classes Clearly)', 
                 fontsize=14, fontweight='bold', pad=20)
    ax.set_yscale('log')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    synthetic_samples = smote_stats.get('synthetic_samples', 0)
    ax.text(0.98, 0.98, f'Synthetic Samples: {synthetic_samples:,}\n(More Balanced)',
            transform=ax.transAxes, ha='right', va='top',
            fontsize=10, bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.8))
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'class_distribution_after_smote_log.png'))
    
    # Plot 2: Linear scale (shows balance better)
    fig, ax = plt.subplots(figsize=(10, 8))
    
    bars = ax.bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)
    
    for i, (cls, bar, count, pct) in enumerate(zip(classes, bars, counts, percentages)):
        height = bar.get_height()
        before_count = dist_before.get(cls, count)
        increase = count - before_count
        factor = count / before_count if before_count > 0 else 0
        
        if increase > 0:
            label = f'{count:,}\n({pct:.2f}%)\n[+{increase:,}]'
            color = 'darkgreen'
        else:
            label = f'{count:,}\n({pct:.2f}%)'
            color = 'black'
        
        ax.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,
                label, ha='center', va='bottom', fontsize=8, fontweight='bold', color=color)
    
    ax.set_ylabel('Number of Samples (Linear Scale)', fontsize=12, fontweight='bold')
    ax.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax.set_title('Class Distribution AFTER SMOTE - Linear Scale\n(Shows Better Balance Achieved)', 
                 fontsize=14, fontweight='bold', pad=20)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, max(counts) * 1.2)
    
    # Calculate new imbalance ratio
    max_count = max(counts)
    min_count = min(counts)
    new_ratio = max_count / min_count
    ax.text(0.98, 0.98, f'New Imbalance: {new_ratio:.1f}:1\n(Much Better!)',
            transform=ax.transAxes, ha='right', va='top',
            fontsize=10, bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.8))
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'class_distribution_after_smote_linear.png'))


def plot_smote_comparison(smote_stats, output_dir):
    """Plot side-by-side comparison of class distribution before and after SMOTE - LINEAR SCALE."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    dist_before = smote_stats.get('distribution_before', {})
    dist_after = smote_stats.get('distribution_after', {})
    classes = sorted(dist_before.keys())
    
    # Calculate max for SAME scale on both plots
    counts_before = [dist_before[c] for c in classes]
    counts_after = [dist_after[c] for c in classes]
    max_count = max(max(counts_before), max(counts_after))
    
    # Before SMOTE
    total_before = sum(counts_before)
    pct_before = [c/total_before*100 for c in counts_before]
    
    colors1 = plt.cm.Reds(np.linspace(0.3, 0.9, len(classes)))
    bars1 = ax1.bar(classes, counts_before, color=colors1, edgecolor='black', linewidth=2)
    
    for bar, count, pct in zip(bars1, counts_before, pct_before):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + max_count*0.01,
                f'{count:,}\n({pct:.2f}%)',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax1.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax1.set_title('BEFORE SMOTE\n(Severely Imbalanced)', fontsize=13, fontweight='bold', pad=15, color='darkred')
    ax1.grid(axis='y', alpha=0.3, linestyle='--')
    ax1.set_ylim(0, max_count * 1.2)
    
    # Add imbalance annotation (exclude __DROP__ class with near-zero count if present)
    valid_counts_before = [c for c in counts_before if c > 100]  # Exclude tiny classes like __DROP__
    imbalance_before = max(valid_counts_before) / min(valid_counts_before) if valid_counts_before else 1
    ax1.text(0.98, 0.98, f'Imbalance:\n{imbalance_before:.0f}:1',
            transform=ax1.transAxes, ha='right', va='top',
            fontsize=11, fontweight='bold',
            bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.9))
    
    # After SMOTE - SAME Y-axis scale for fair comparison
    total_after = sum(counts_after)
    pct_after = [c/total_after*100 for c in counts_after]
    
    colors2 = plt.cm.Greens(np.linspace(0.3, 0.9, len(classes)))
    bars2 = ax2.bar(classes, counts_after, color=colors2, edgecolor='black', linewidth=2)
    
    for cls, bar, count, pct in zip(classes, bars2, counts_after, pct_after):
        height = bar.get_height()
        before_count = dist_before.get(cls, count)
        increase = count - before_count
        factor = count / before_count if before_count > 0 else 0
        
        if increase > 0:
            label = f'{count:,}\n({pct:.2f}%)\n[+{increase:,}, {factor:.1f}x]'
            color = 'darkgreen'
        else:
            label = f'{count:,}\n({pct:.2f}%)'
            color = 'black'
        
        ax2.text(bar.get_x() + bar.get_width()/2., height + max_count*0.01,
                label, ha='center', va='bottom', fontsize=8, fontweight='bold', color=color)
    
    ax2.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Class', fontsize=12, fontweight='bold')
    ax2.set_title('AFTER SMOTE\n(Much More Balanced)', fontsize=13, fontweight='bold', pad=15, color='darkgreen')
    ax2.grid(axis='y', alpha=0.3, linestyle='--')
    ax2.set_ylim(0, max_count * 1.2)
    
    # Add balance improvement annotation (exclude __DROP__ class with near-zero count if present)
    valid_counts_after = [c for c in counts_after if c > 100]  # Exclude tiny classes like __DROP__
    imbalance_after = max(valid_counts_after) / min(valid_counts_after) if valid_counts_after else 1
    improvement = imbalance_before / imbalance_after
    ax2.text(0.98, 0.98, f'Imbalance:\n{imbalance_after:.1f}:1\n({improvement:.0f}x better!)',
            transform=ax2.transAxes, ha='right', va='top',
            fontsize=11, fontweight='bold',
            bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.9))
    
    plt.suptitle('SMOTE Effect: Class Distribution Comparison (SAME Scale)', 
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'smote_comparison_linear.png'))


def plot_rfe_feature_importance(rfe_stats, output_dir):
    """Plot feature importance of selected features after RFE."""
    if 'selected_features' not in rfe_stats or len(rfe_stats['selected_features']) == 0:
        return
    
    fig, ax = plt.subplots(figsize=(12, 10))
    
    features = rfe_stats['selected_features'][:30]  # Top 30
    n_features = len(features)
    
    # Use actual importance values from the feature importance CSV if available
    # Otherwise fall back to descending rank-based values
    importance_file = os.path.join(output_dir, '..', 'preprocessed', 'feature_importances.csv')
    try:
        import pandas as pd
        imp_df = pd.read_csv(importance_file)
        imp_dict = dict(zip(imp_df['feature'], imp_df['importance']))
        importances = [imp_dict.get(f, 0) for f in features]
    except Exception:
        importances = list(range(n_features, 0, -1))  # Rank-based fallback
    
    colors = plt.cm.viridis(np.linspace(0, 1, n_features))
    bars = ax.barh(range(n_features), importances, color=colors, edgecolor='black', linewidth=1)
    
    ax.set_yticks(range(n_features))
    ax.set_yticklabels(features, fontsize=9)
    ax.set_xlabel('Relative Importance', fontsize=12, fontweight='bold')
    ax.set_ylabel('Features', fontsize=12, fontweight='bold')
    ax.set_title(f'Top {n_features} Selected Features After RFE', fontsize=14, fontweight='bold', pad=20)
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    
    # Add note
    total_features = rfe_stats.get('n_features_selected', n_features)
    ax.text(0.98, 0.02, f'Total Selected: {total_features} features',
            transform=ax.transAxes, ha='right', va='bottom',
            fontsize=10, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'rfe_selected_features.png'))


def plot_rfe_performance_curve(rfe_stats, output_dir):
    """Plot RFE performance curve showing F1 score vs number of features."""
    if 'cv_results' not in rfe_stats:
        return
    
    fig, ax = plt.subplots(figsize=(12, 7))
    
    cv_results = rfe_stats['cv_results']
    n_features_range = range(len(cv_results['mean_test_score']))
    mean_scores = cv_results['mean_test_score']
    std_scores = cv_results.get('std_test_score', [0] * len(mean_scores))
    
    # Plot mean score with std deviation band
    ax.plot(n_features_range, mean_scores, 'b-', linewidth=2, label='Mean F1-macro')
    ax.fill_between(n_features_range, 
                     np.array(mean_scores) - np.array(std_scores),
                     np.array(mean_scores) + np.array(std_scores),
                     alpha=0.2, color='blue', label='±1 std dev')
    
    # Mark optimal point
    optimal_idx = np.argmax(mean_scores)
    optimal_score = mean_scores[optimal_idx]
    ax.plot(optimal_idx, optimal_score, 'r*', markersize=20, 
            label=f'Optimal: {optimal_idx+1} features, F1={optimal_score:.4f}')
    
    ax.set_xlabel('Number of Features', fontsize=12, fontweight='bold')
    ax.set_ylabel('F1-Macro Score (Cross-Validation)', fontsize=12, fontweight='bold')
    ax.set_title('RFE Performance: F1-Score vs Number of Features', fontsize=14, fontweight='bold', pad=20)
    ax.grid(alpha=0.3, linestyle='--')
    ax.legend(loc='best', fontsize=10)
    
    # Add annotation
    ax.annotate(f'Optimal: {optimal_idx+1} features',
                xy=(optimal_idx, optimal_score), xytext=(optimal_idx+5, optimal_score-0.01),
                arrowprops=dict(arrowstyle='->', color='red', lw=2),
                fontsize=11, fontweight='bold', color='red')
    
    plt.tight_layout()
    save_figure(fig, os.path.join(output_dir, 'rfe_performance_curve.png'))


def generate_preprocessing_report(all_stats, output_dir):
    """Generate comprehensive preprocessing report."""
    from ml_model.utils import write_text_report, format_number
    
    lines = []
    lines.append("=" * 80)
    lines.append(" " * 20 + "DATA PREPROCESSING REPORT")
    lines.append(" " * 20 + "CICIDS2018 Dataset")
    lines.append(" " * 15 + f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("=" * 80)
    lines.append("")
    
    # STEP 1: DATA CLEANING
    if 'cleaning' in all_stats:
        clean = all_stats['cleaning']
        lines.append("1. DATA CLEANING")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Initial Dataset:")
        lines.append(f"     Rows: {format_number(clean.get('initial_rows', 0))}")
        lines.append(f"     Columns: {clean.get('initial_cols', 0)}")
        lines.append(f"     Memory: {clean.get('initial_memory_gb', 0):.2f} GB")
        lines.append("")
        
        lines.append(f"   Data Quality Issues Found:")
        lines.append(f"     Rows with NaN: {format_number(clean.get('nan_rows', 0))} ({clean.get('nan_percentage', 0):.3f}%)")
        lines.append(f"     Columns with NaN: {clean.get('nan_cols', 0)}")
        lines.append(f"     Rows with Inf: {format_number(clean.get('inf_rows', 0))} ({clean.get('inf_percentage', 0):.3f}%)")
        lines.append(f"     Columns with Inf: {clean.get('inf_cols', 0)}")
        lines.append(f"     Duplicate rows: {format_number(clean.get('duplicate_rows', 0))} ({clean.get('duplicate_percentage', 0):.3f}%)")
        lines.append("")
        
        lines.append(f"   Cleaning Actions:")
        dropped_cols = clean.get('dropped_columns', [])
        if dropped_cols:
            lines.append(f"     ✓ Removed useless columns: {', '.join(dropped_cols)}")
        else:
            lines.append(f"     ✓ Removed useless columns: None (already removed by data loader)")
        lines.append(f"     ✓ Removed bad 'Label' class: {format_number(clean.get('label_class_removed', 0))} rows (header misplacement)")
        lines.append(f"     ✓ Removed all NaN rows: {format_number(clean.get('nan_rows', 0))}")
        lines.append(f"     ✓ Removed all Inf rows: {format_number(clean.get('inf_rows', 0))}")
        lines.append(f"     ✓ Removed duplicate rows: {format_number(clean.get('duplicate_rows', 0))}")
        lines.append("")
        
        lines.append(f"   Final Clean Dataset:")
        lines.append(f"     Rows: {format_number(clean.get('final_rows', 0))}")
        lines.append(f"     Columns: {clean.get('final_cols', 0)}")
        lines.append(f"     Memory: {clean.get('final_memory_gb', 0):.2f} GB")
        lines.append(f"     Total removed: {format_number(clean.get('total_removed', 0))} rows ({clean.get('removal_percentage', 0):.3f}%)")
        lines.append(f"     Memory saved: {clean.get('memory_saved_gb', 0):.2f} GB")
        lines.append("")
        removal_pct = clean.get('removal_percentage', 0)
        if removal_pct < 1.0:
            assessment = '✓ EXCELLENT'
            threshold_msg = '<1%'
        elif removal_pct < 5.0:
            assessment = '✓ ACCEPTABLE'
            threshold_msg = '<5%'
        else:
            assessment = '⚠ WARNING - HIGH DATA LOSS'
            threshold_msg = '>5% (EXCEEDS THRESHOLD)'
        
        lines.append(f"   Quality Assessment: {assessment}")
        lines.append(f"   Data loss {threshold_msg}: {removal_pct:.3f}%")
        lines.append("")
    
    # STEP 2: LABEL CONSOLIDATION
    if 'consolidation' in all_stats:
        consol = all_stats['consolidation']
        lines.append("2. LABEL CONSOLIDATION")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Consolidation Strategy: Merge attack subcategories into parent classes")
        lines.append(f"     Original classes: {consol.get('original_classes', 0)}")
        lines.append(f"     Consolidated classes: {consol.get('consolidated_classes', 0)}")
        lines.append(f"     Reduction: {consol.get('original_classes', 0) - consol.get('consolidated_classes', 0)} classes")
        lines.append("")
        
        lines.append(f"   Consolidation Mapping (from config.LABEL_MAPPING):")
        # Build mapping dynamically from config
        mapping_groups = {}
        dropped_classes = []
        for src, dst in config.LABEL_MAPPING.items():
            if dst == '__DROP__':
                dropped_classes.append(src)
            elif src != dst:
                if dst not in mapping_groups:
                    mapping_groups[dst] = []
                mapping_groups[dst].append(src)
        for target, sources in sorted(mapping_groups.items()):
            lines.append(f"     • {', '.join(sources)} → {target}")
        if dropped_classes:
            lines.append(f"     • {', '.join(dropped_classes)} → DROPPED")
        lines.append("")
        
        lines.append(f"   Final Class Distribution:")
        if 'consolidated_distribution' in consol:
            for label, count in sorted(consol['consolidated_distribution'].items(), key=lambda x: x[1], reverse=True):
                pct = count / sum(consol['consolidated_distribution'].values()) * 100
                lines.append(f"     {label:20s}: {format_number(count):>12s} ({pct:>6.2f}%)")
        lines.append("")
    
    # STEP 3: CATEGORICAL ENCODING
    if 'encoding' in all_stats:
        enc = all_stats['encoding']
        lines.append("3. CATEGORICAL ENCODING")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Encoding Methods Applied:")
        lines.append(f"     • Protocol column: One-hot encoding (creates binary columns)")
        lines.append(f"     • Target (Label): Label encoding ({enc.get('n_classes', 0)} classes → integers 0-{enc.get('n_classes', 1) - 1})")
        lines.append("")
        
        lines.append(f"   Protocol One-Hot Encoding:")
        if enc.get('protocol_columns'):
            for col in enc['protocol_columns']:
                lines.append(f"     ✓ Created: {col}")
            lines.append(f"     Total columns created: {len(enc['protocol_columns'])}")
        else:
            lines.append(f"     No Protocol column found - skipped")
        lines.append("")
        
        lines.append(f"   Target Label Encoding:")
        lines.append(f"     Classes encoded: {enc.get('n_classes', 0)}")
        if 'class_mapping' in enc:
            for idx, label in enc['class_mapping'].items():
                lines.append(f"       {idx}: {label}")
        lines.append("")
        
        lines.append(f"   Columns After Encoding:")
        lines.append(f"     Before: {enc.get('original_columns', 0)}")
        lines.append(f"     After: {enc.get('encoded_columns', 0)}")
        lines.append(f"     Added: +{enc.get('columns_added', 0)}")
        lines.append(f"     ✓ All categorical features converted to numerical")
        lines.append("")
    
    # STEP 4: TRAIN-TEST SPLIT
    if 'split' in all_stats:
        split = all_stats['split']
        lines.append("4. TRAIN-TEST SPLIT")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Split Configuration:")
        lines.append(f"     Method: Stratified split (maintains class proportions)")
        lines.append(f"     Ratio: {split.get('train_percentage', 0):.1f}% train / {split.get('test_percentage', 0):.1f}% test")
        lines.append(f"     Random seed: {split.get('random_state', 42)}")
        lines.append("")
        
        lines.append(f"   Dataset Sizes:")
        lines.append(f"     Total samples: {format_number(split.get('total_samples', 0))}")
        lines.append(f"     Features: {split.get('n_features', 0)}")
        lines.append(f"     Training set: {format_number(split.get('n_train', 0))} samples")
        lines.append(f"     Test set: {format_number(split.get('n_test', 0))} samples")
        lines.append("")
        
        lines.append(f"   Stratification Verification:")
        lines.append(f"     Stratified: {'✓ Yes' if split.get('stratified', True) else '✗ No'}")
        lines.append(f"     Max class distribution difference: {split.get('max_distribution_diff', 0)*100:.3f}%")
        lines.append(f"     {'✓ VERIFIED' if split.get('max_distribution_diff', 0) < 0.01 else '⚠ WARNING'}: Train and test have {'same' if split.get('max_distribution_diff', 0) < 0.01 else 'different'} class proportions")
        lines.append("")
    
    # STEP 5: FEATURE SCALING
    if 'scaling' in all_stats:
        scale = all_stats['scaling']
        lines.append("5. FEATURE SCALING")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Scaling Method: {scale.get('scaler_type', 'standard').upper()}")
        if scale.get('scaler_type') == 'standard':
            lines.append(f"     StandardScaler: Transforms features to mean=0, std=1")
            lines.append(f"     Formula: (x - mean) / std")
        else:
            lines.append(f"     MinMaxScaler: Transforms features to range [0, 1]")
            lines.append(f"     Formula: (x - min) / (max - min)")
        lines.append("")
        
        lines.append(f"   Scaling Process:")
        lines.append(f"     1. Scaler fitted on TRAINING data ONLY")
        lines.append(f"     2. Training data transformed using learned parameters")
        lines.append(f"     3. Test data transformed using TRAINING parameters")
        lines.append(f"     ✓ Data leakage PREVENTED (test data never influenced scaler)")
        lines.append("")
        
        lines.append(f"   Features Scaled:")
        lines.append(f"     Number: {scale.get('n_features', 0)}")
        lines.append(f"     Training samples: {format_number(scale.get('train_shape', (0,))[0])}")
        lines.append(f"     Test samples: {format_number(scale.get('test_shape', (0,))[0])}")
        lines.append("")
    
    # STEP 6: SMOTE
    if 'smote' in all_stats:
        smote = all_stats['smote']
        if smote.get('applied', True):
            lines.append("6. CLASS IMBALANCE HANDLING (SMOTE)")
            lines.append("   " + "=" * 70)
            lines.append("")
            lines.append(f"   SMOTE Configuration:")
            smote_strategy = smote.get('strategy', config.SMOTE_STRATEGY)
            if smote_strategy == 'dynamic':
                lines.append(f"     Strategy: Dynamic oversampling (bring minorities halfway to 2nd largest class)")
            elif smote_strategy == 'tiered':
                lines.append(f"     Strategy: Tiered oversampling (config-based targets per class)")
            else:
                lines.append(f"     Strategy: Uniform oversampling ({smote.get('target_percentage', 0)*100:.1f}% of dataset)")
            lines.append(f"     k_neighbors: {smote.get('k_neighbors', 5)}")
            lines.append(f"     Applied to: TRAINING data only (test remains imbalanced)")
            lines.append(f"     Reason: Simulates real-world deployment conditions")
            lines.append("")
            
            lines.append(f"   Class Distribution Changes:")
            lines.append(f"     Classes oversampled: {smote.get('classes_oversampled', 0)}")
            if 'distribution_before' in smote and 'distribution_after' in smote:
                lines.append("")
                lines.append(f"     Before SMOTE:")
                for cls, count in sorted(smote['distribution_before'].items()):
                    pct = count / smote['before_count'] * 100
                    lines.append(f"       Class {cls}: {format_number(count):>10s} ({pct:>6.2f}%)")
                lines.append("")
                lines.append(f"     After SMOTE:")
                for cls, count in sorted(smote['distribution_after'].items()):
                    pct = count / smote['after_count'] * 100
                    before_count = smote['distribution_before'].get(cls, count)
                    increase = count - before_count
                    factor = count / before_count if before_count > 0 else 0
                    if increase > 0:
                        lines.append(f"       Class {cls}: {format_number(count):>10s} ({pct:>6.2f}%) [+{format_number(increase)}, {factor:.1f}x]")
                    else:
                        lines.append(f"       Class {cls}: {format_number(count):>10s} ({pct:>6.2f}%)")
            lines.append("")
            
            lines.append(f"   SMOTE Summary:")
            lines.append(f"     Samples before: {format_number(smote.get('before_count', 0))}")
            lines.append(f"     Samples after: {format_number(smote.get('after_count', 0))}")
            lines.append(f"     Synthetic samples: {format_number(smote.get('synthetic_samples', 0))}")
            lines.append(f"     Increase: {(smote.get('after_count', 0) - smote.get('before_count', 0)) / smote.get('before_count', 1) * 100:.2f}%")
            lines.append("")
        else:
            lines.append("6. CLASS IMBALANCE HANDLING (SMOTE)")
            lines.append("   " + "=" * 70)
            lines.append("")
            lines.append(f"   Status: DISABLED")
            lines.append(f"   Reason: APPLY_SMOTE=False in config.py")
            lines.append("")
    
    # STEP 6.5: CORRELATION-BASED FEATURE ELIMINATION
    if 'correlation_elimination' in all_stats:
        corr_elim = all_stats['correlation_elimination']
        lines.append("6.5. CORRELATION-BASED FEATURE ELIMINATION")
        lines.append("   " + "=" * 70)
        lines.append("")
        lines.append(f"   Correlation Elimination Configuration:")
        lines.append(f"     Method: Remove highly correlated feature pairs")
        lines.append(f"     Correlation threshold: {corr_elim.get('corr_threshold', 0.99)}")
        lines.append(f"     Strategy: Remove one feature from each |r| ≥ {corr_elim.get('corr_threshold', config.CORR_ELIMINATION_THRESHOLD)} pair")
        lines.append("")
        
        lines.append(f"   Correlation Analysis Results:")
        lines.append(f"     Highly correlated pairs found: {corr_elim.get('n_pairs_found', 0)}")
        lines.append(f"     Features analyzed: {corr_elim.get('n_features_before', 0)}")
        lines.append(f"     Features removed: {corr_elim.get('n_features_removed', 0)}")
        lines.append(f"     Features retained: {corr_elim.get('n_features_after', 0)}")
        lines.append(f"     Reduction: {(corr_elim.get('n_features_removed', 0) / corr_elim.get('n_features_before', 1) * 100):.1f}%")
        lines.append("")
        
        if corr_elim.get('n_features_removed', 0) > 0:
            lines.append(f"   Top Removed Features (redundant):")
            if 'removed_features' in corr_elim:
                removed_list = corr_elim['removed_features'][:10]
                for i, feat in enumerate(removed_list, 1):
                    lines.append(f"     {i:2d}. {feat}")
                if len(corr_elim['removed_features']) > 10:
                    lines.append(f"     ... and {len(corr_elim['removed_features']) - 10} more")
            lines.append("")
            
            if 'correlated_pairs' in corr_elim:
                lines.append(f"   Top Correlated Pairs (highest |r|):")
                pairs_list = sorted(corr_elim['correlated_pairs'], key=lambda x: abs(x[2]), reverse=True)[:10]
                for i, (feat1, feat2, corr) in enumerate(pairs_list, 1):
                    lines.append(f"     {i:2d}. {feat1} ↔ {feat2}: r = {corr:7.5f}")
                if len(corr_elim['correlated_pairs']) > 10:
                    lines.append(f"     ... and {len(corr_elim['correlated_pairs']) - 10} more pairs")
                lines.append("")
        else:
            lines.append(f"   Result: No feature pairs with |r| ≥ {corr_elim.get('corr_threshold', config.CORR_ELIMINATION_THRESHOLD)} found")
            lines.append(f"   Impact: All features retained (no correlation-based elimination occurred)")
            lines.append("")
    
    # STEP 7: FEATURE SELECTION (RFE)
    if 'rfe' in all_stats:
        rfe = all_stats['rfe']
        if rfe.get('applied', True):
            lines.append("7. FEATURE SELECTION (RANDOM FOREST IMPORTANCE)")
            lines.append("   " + "=" * 70)
            lines.append("")
            lines.append(f"   RF Importance Configuration:")
            lines.append(f"     Method: Random Forest Gini Importance")
            lines.append(f"     Estimator: Random Forest (n_estimators={config.RF_IMPORTANCE_TREES}, max_depth={config.RF_IMPORTANCE_MAX_DEPTH})")
            lines.append(f"     Target features: {config.TARGET_FEATURES_MIN}-{config.TARGET_FEATURES_MAX} (moderate selection)")
            lines.append("")
            
            lines.append(f"   Feature Selection Results:")
            lines.append(f"     Original features: {rfe.get('n_features_before', 0)}")
            lines.append(f"     Selected features: {rfe.get('n_features_selected', 0)}")
            lines.append(f"     Reduction: {rfe.get('reduction_percentage', 0):.1f}%")
            lines.append(f"     Importance threshold: {rfe.get('importance_threshold', 0.0001):.6f}")
            lines.append("")
            
            lines.append(f"   Selected Features ({rfe.get('n_features_selected', 0)}):")
            if 'selected_features' in rfe:
                for i, feat in enumerate(rfe['selected_features'][:20], 1):
                    lines.append(f"     {i:2d}. {feat}")
                if len(rfe['selected_features']) > 20:
                    lines.append(f"     ... and {len(rfe['selected_features']) - 20} more")
            lines.append("")
            
            lines.append(f"   Performance Impact:")
            lines.append(f"     ✓ Reduced model complexity by {rfe.get('reduction_percentage', 0):.1f}%")
            lines.append(f"     ✓ Removed low-importance features (threshold < {rfe.get('importance_threshold', 0.0001):.6f})")
            lines.append(f"     ✓ Faster inference time (fewer features)")
            lines.append("")
        else:
            lines.append("7. FEATURE SELECTION (RF IMPORTANCE)")
            lines.append("   " + "=" * 70)
            lines.append("")
            lines.append(f"   Status: DISABLED")
            lines.append(f"   Reason: ENABLE_RF_IMPORTANCE=False in config.py")
            lines.append(f"   To enable: Set ENABLE_RF_IMPORTANCE=True in config.py")
            lines.append("")
    
    # FINAL SUMMARY
    lines.append("=" * 80)
    lines.append(" " * 25 + "FINAL PREPROCESSED DATASET")
    lines.append("=" * 80)
    lines.append("")
    
    if 'split' in all_stats and 'rfe' in all_stats:
        lines.append(f"Training Set:")
        if all_stats['smote'].get('applied', True):
            lines.append(f"  Samples: {format_number(all_stats['smote'].get('after_count', 0))} (after SMOTE)")
        else:
            lines.append(f"  Samples: {format_number(all_stats['split'].get('n_train', 0))}")
        
        if all_stats['rfe'].get('applied', True):
            lines.append(f"  Features: {all_stats['rfe'].get('n_features_selected', 0)} (after feature selection)")
        else:
            lines.append(f"  Features: {all_stats['split'].get('n_features', 0)}")
        
        lines.append(f"  Classes: {all_stats['encoding'].get('n_classes', 0)}")
        lines.append("")
        
        lines.append(f"Test Set:")
        lines.append(f"  Samples: {format_number(all_stats['split'].get('n_test', 0))} (original distribution)")
        if all_stats['rfe'].get('applied', True):
            lines.append(f"  Features: {all_stats['rfe'].get('n_features_selected', 0)} (same as train)")
        else:
            lines.append(f"  Features: {all_stats['split'].get('n_features', 0)}")
        lines.append(f"  Classes: {all_stats['encoding'].get('n_classes', 0)}")
        lines.append("")
    
    lines.append("=" * 80)
    lines.append(" " * 22 + "PREPROCESSING QUALITY ASSESSMENT")
    lines.append("=" * 80)
    lines.append("")
    lines.append(f"Data Cleaning:")
    if 'cleaning' in all_stats:
        lines.append(f"  ✓ NaN values removed ({all_stats['cleaning'].get('nan_percentage', 0):.3f}%)")
        lines.append(f"  ✓ Inf values removed ({all_stats['cleaning'].get('inf_percentage', 0):.3f}%)")
        lines.append(f"  ✓ Duplicates removed ({all_stats['cleaning'].get('duplicate_percentage', 0):.3f}%)")
        removal_pct = all_stats['cleaning'].get('removal_percentage', 0)
        loss_mark = "✓" if removal_pct < 5.0 else "⚠"
        loss_label = "acceptable" if removal_pct < 5.0 else "high"
        lines.append(f"  {loss_mark} Data loss: {removal_pct:.3f}% ({loss_label})")
    lines.append("")
    
    lines.append(f"Label Consolidation:")
    if 'consolidation' in all_stats:
        lines.append(f"  ✓ {all_stats['consolidation'].get('original_classes', 0)} classes → {all_stats['consolidation'].get('consolidated_classes', 0)} classes")
        lines.append(f"  ✓ Consistent naming applied")
        lines.append(f"  ✓ No unmapped labels")
    lines.append("")
    
    lines.append(f"Encoding:")
    lines.append(f"  ✓ All categorical features converted to numerical")
    if 'encoding' in all_stats:
        lines.append(f"  ✓ Protocol one-hot encoded ({len(all_stats['encoding'].get('protocol_columns', []))} columns)")
        lines.append(f"  ✓ Target label encoded ({all_stats['encoding'].get('n_classes', 0)} classes)")
    lines.append("")
    
    lines.append(f"Train-Test Split:")
    if 'split' in all_stats:
        lines.append(f"  ✓ {all_stats['split'].get('train_percentage', 0):.0f}:{all_stats['split'].get('test_percentage', 0):.0f} ratio achieved")
        lines.append(f"  ✓ Stratification verified (max diff {all_stats['split'].get('max_distribution_diff', 0)*100:.3f}%)")
        lines.append(f"  ✓ Reproducible (random_state={all_stats['split'].get('random_state', 42)})")
    lines.append("")
    
    lines.append(f"Feature Scaling:")
    if 'scaling' in all_stats:
        lines.append(f"  ✓ {all_stats['scaling'].get('scaler_type', 'standard').title()}Scaler applied")
        lines.append(f"  ✓ Fitted on training data only")
        lines.append(f"  ✓ No data leakage")
    lines.append("")
    
    lines.append(f"Class Imbalance:")
    if 'smote' in all_stats and all_stats['smote'].get('applied', True):
        lines.append(f"  ✓ SMOTE applied to training set")
        lines.append(f"  ✓ {all_stats['smote'].get('strategy', config.SMOTE_STRATEGY).title()} oversampling ({format_number(all_stats['smote'].get('synthetic_samples', 0))} synthetic samples)")
        lines.append(f"  ✓ Test set remains imbalanced (real-world simulation)")
    else:
        lines.append(f"  ✗ SMOTE not applied")
    lines.append("")
    
    lines.append(f"Feature Selection:")
    if 'rfe' in all_stats and all_stats['rfe'].get('applied', True):
        lines.append(f"  ✓ RF Gini Importance completed")
        lines.append(f"  ✓ {all_stats['rfe'].get('n_features_selected', 0)} optimal features selected")
        lines.append(f"  ✓ Reduced complexity by {all_stats['rfe'].get('reduction_percentage', 0):.1f}%")
    else:
        lines.append(f"  ✗ RF Importance not applied (all features retained)")
    lines.append("")
    
    lines.append("=" * 80)
    removal_pct = all_stats.get('cleaning', {}).get('removal_percentage', 0)
    smote_ok = all_stats.get('smote', {}).get('applied', False)
    rfe_ok = 'rfe' in all_stats and all_stats.get('rfe', {}).get('applied', True)
    if removal_pct < 1.0 and smote_ok and rfe_ok:
        lines.append("Overall Assessment: ✓✓✓ EXCELLENT")
    elif removal_pct < 30.0:
        lines.append("Overall Assessment: ✓✓ GOOD")
    else:
        lines.append("Overall Assessment: ⚠ NEEDS REVIEW")
    readiness = "Ready for model training" if removal_pct < 50.0 else "Review recommended before training"
    lines.append(f"Data Quality: {readiness}")
    lines.append(f"Expected Performance: >{config.TARGET_MACRO_F1_SCORE*100:.0f}% macro F1-score (target)")
    lines.append("=" * 80)
    lines.append("")
    lines.append(f"Report generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"Module: Data Preprocessing (Module 3)")
    lines.append(f"Next step: Model Training (Module 4)")
    lines.append("")
    
    report_path = os.path.join(output_dir, 'preprocessing_results.txt')
    write_text_report('\n'.join(lines), report_path)
    log_message(f"✓ Saved detailed report: preprocessing_results.txt", level="SUCCESS")


def generate_preprocessing_steps_log(all_stats, output_dir):
    """Generate step-by-step preprocessing log."""
    from ml_model.utils import write_text_report, format_number
    
    lines = []
    lines.append("=" * 80)
    lines.append(" " * 18 + "MODULE 3: DATA PREPROCESSING")
    lines.append(" " * 23 + "DETAILED STEP-BY-STEP LOG")
    lines.append(" " * 15 + f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("=" * 80)
    lines.append("")
    lines.append("This log provides a detailed chronological record of every preprocessing")
    lines.append("operation performed, including before/after states, actions taken, and")
    lines.append("validation checks.")
    lines.append("")
    lines.append("=" * 80)
    lines.append("")
    
    step_num = 1
    
    # STEP 1: DATA CLEANING
    if 'cleaning' in all_stats:
        clean = all_stats['cleaning']
        lines.append(f"STEP {step_num}: DATA CLEANING")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Remove poor quality data (NaN, Inf, duplicates) and useless columns")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.1] Initial State Assessment")
        lines.append(f"  • Dataset shape: {format_number(clean.get('initial_rows', 0))} rows × {clean.get('initial_cols', 0)} columns")
        lines.append(f"  • Memory usage: {clean.get('initial_memory_gb', 0):.2f} GB")
        lines.append(f"  • Action: Assess data quality issues")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.2] Remove Useless Columns")
        lines.append(f"  • Before: {clean.get('initial_cols', 0)} columns")
        dropped_cols = clean.get('dropped_columns', [])
        lines.append(f"  • Columns dropped: {len(dropped_cols)}")
        if dropped_cols:
            lines.append(f"    Dropped: {', '.join(dropped_cols)}")
        lines.append(f"  • Reason: Not useful for ML (identifiers, not features)")
        lines.append(f"  • Action: df.drop(columns={dropped_cols}, errors='ignore')")
        lines.append(f"  • After: {clean.get('initial_cols', 0) - len(dropped_cols)} columns")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.3] Remove Bad 'Label' Class")
        lines.append(f"  • Issue: Rows with label = 'Label' (misplaced header)")
        lines.append(f"  • Action: df = df[df['Label'] != 'Label']")
        lines.append(f"  • Rows removed: {clean.get('label_class_removed', 0)}")
        lines.append(f"  • Validation: No 'Label' values in Label column")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.4] Identify NaN Values")
        lines.append(f"  • Rows with NaN: {format_number(clean.get('nan_rows', 0))} ({clean.get('nan_percentage', 0):.3f}%)")
        lines.append(f"  • Columns affected: {clean.get('nan_cols', 0)}")
        lines.append(f"  • Decision: Remove all NaN rows (data loss acceptable)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.5] Remove NaN Values")
        rows_after_bad_label = clean.get('initial_rows', 0) - clean.get('label_class_removed', 0)
        lines.append(f"  • Before: {format_number(rows_after_bad_label)} rows")
        lines.append(f"  • Action: df.dropna(inplace=True)")
        lines.append(f"  • Removed: {format_number(clean.get('nan_rows', 0))} rows")
        lines.append(f"  • After: {format_number(rows_after_bad_label - clean.get('nan_rows', 0))} rows")
        lines.append(f"  • Validation: df.isna().sum().sum() == 0 ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.6] Identify Infinite Values")
        lines.append(f"  • Rows with Inf: {format_number(clean.get('inf_rows', 0))} ({clean.get('inf_percentage', 0):.3f}%)")
        lines.append(f"  • Columns affected: {clean.get('inf_cols', 0)}")
        lines.append(f"  • Decision: Remove all Inf rows (data loss acceptable)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.7] Remove Infinite Values")
        rows_after_nan = clean.get('initial_rows', 0) - clean.get('label_class_removed', 0) - clean.get('nan_rows', 0)
        rows_after_inf = rows_after_nan - clean.get('inf_rows', 0)
        lines.append(f"  • Before: {format_number(rows_after_nan)} rows")
        lines.append(f"  • Action: df = df[~df.isin([np.inf, -np.inf]).any(axis=1)]")
        lines.append(f"  • Removed: {format_number(clean.get('inf_rows', 0))} rows")
        lines.append(f"  • After: {format_number(rows_after_inf)} rows")
        lines.append(f"  • Validation: np.isinf(df.select_dtypes(include=[np.number])).sum().sum() == 0 ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.8] Identify Duplicate Rows")
        lines.append(f"  • Duplicate rows: {format_number(clean.get('duplicate_rows', 0))} ({clean.get('duplicate_percentage', 0):.3f}%)")
        lines.append(f"  • Decision: Remove all duplicates (keep first occurrence)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.9] Remove Duplicate Rows")
        rows_before_dup = clean.get('initial_rows', 0) - clean.get('label_class_removed', 0) - clean.get('nan_rows', 0) - clean.get('inf_rows', 0)
        lines.append(f"  • Before: {format_number(rows_before_dup)} rows")
        lines.append(f"  • Action: df.drop_duplicates(inplace=True)")
        lines.append(f"  • Removed: {format_number(clean.get('duplicate_rows', 0))} rows")
        lines.append(f"  • After: {format_number(clean.get('final_rows', 0))} rows")
        lines.append(f"  • Validation: df.duplicated().sum() == 0 ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 1.10] Final Cleaning Summary")
        lines.append(f"  • Initial rows: {format_number(clean.get('initial_rows', 0))}")
        lines.append(f"  • Final rows: {format_number(clean.get('final_rows', 0))}")
        lines.append(f"  • Total removed: {format_number(clean.get('total_removed', 0))} ({clean.get('removal_percentage', 0):.3f}%)")
        lines.append(f"  • Memory saved: {clean.get('memory_saved_gb', 0):.2f} GB")
        lines.append(f"  • Quality: {'✓ EXCELLENT' if clean.get('removal_percentage', 0) < 1.0 else '✓ ACCEPTABLE'}")
        lines.append(f"  • Status: Cleaning complete, proceeding to consolidation")
        lines.append("")
        step_num += 1
    
    # STEP 2: LABEL CONSOLIDATION
    if 'consolidation' in all_stats:
        consol = all_stats['consolidation']
        lines.append(f"STEP {step_num}: LABEL CONSOLIDATION")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Merge attack subcategories into parent classes")
        lines.append("")
        
        lines.append(f"[SUBSTEP 2.1] Analyze Original Labels")
        lines.append(f"  • Unique labels found: {consol.get('original_classes', 0)}")
        lines.append(f"  • Action: df['Label'].value_counts()")
        if 'original_distribution' in consol:
            lines.append(f"  • Distribution:")
            for label, count in sorted(consol['original_distribution'].items(), key=lambda x: x[1], reverse=True):
                pct = count / sum(consol['original_distribution'].values()) * 100
                lines.append(f"      {label:25s}: {format_number(count):>10s} ({pct:>5.2f}%)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 2.2] Define Consolidation Mapping")
        lines.append(f"  • Mapping strategy: Group similar attacks")
        lines.append(f"  • Mappings defined in config.LABEL_MAPPING:")
        # Build mapping dynamically from config
        stl_mapping_groups = {}
        stl_dropped = []
        for src, dst in config.LABEL_MAPPING.items():
            if dst == '__DROP__':
                stl_dropped.append(src)
            elif src != dst:
                if dst not in stl_mapping_groups:
                    stl_mapping_groups[dst] = []
                stl_mapping_groups[dst].append(src)
        for target, sources in sorted(stl_mapping_groups.items()):
            lines.append(f"      {', '.join(sources)} → {target}")
        if stl_dropped:
            lines.append(f"      {', '.join(stl_dropped)} → DROPPED")
        lines.append("")
        
        lines.append(f"[SUBSTEP 2.3] Apply Label Mapping")
        lines.append(f"  • Action: df['Label'] = df['Label'].map(config.LABEL_MAPPING).fillna(df['Label'])")
        lines.append(f"  • Before: {consol.get('original_classes', 0)} classes")
        lines.append(f"  • Mapped to: {consol.get('consolidated_classes', 0)} classes (includes __DROP__)")
        lines.append("")
        
        dropped_rows = consol.get('dropped_rows', 0)
        if dropped_rows > 0:
            lines.append(f"[SUBSTEP 2.4] Remove Rows Marked for Dropping")
            lines.append(f"  • Rows marked __DROP__ (e.g., SQL Injection): {format_number(dropped_rows)}")
            lines.append(f"  • Action: df = df[df['Label'] != '__DROP__']")
            lines.append(f"  • Removed: {format_number(dropped_rows)} rows")
            lines.append(f"  • After: {consol.get('consolidated_classes', 0) - 1} classes (final)")
            lines.append("")
        
        lines.append(f"[SUBSTEP 2.5] Verify Consolidated Labels")
        if 'consolidated_distribution' in consol:
            lines.append(f"  • Final distribution:")
            for label, count in sorted(consol['consolidated_distribution'].items(), key=lambda x: x[1], reverse=True):
                pct = count / sum(consol['consolidated_distribution'].values()) * 100
                lines.append(f"      {label:20s}: {format_number(count):>10s} ({pct:>5.2f}%)")
        lines.append(f"  • Validation: All labels mapped correctly ✓")
        lines.append("")
        step_num += 1
    
    # STEP 3: CATEGORICAL ENCODING
    if 'encoding' in all_stats:
        enc = all_stats['encoding']
        lines.append(f"STEP {step_num}: CATEGORICAL ENCODING")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Convert categorical features to numerical (required for ML)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 3.1] Identify Categorical Columns")
        lines.append(f"  • Searching for: Protocol column")
        lines.append(f"  • Searching for: Dst Port column")
        lines.append(f"  • Found: {len(enc.get('protocol_columns', [])) > 0}")
        lines.append("")
        
        if enc.get('protocol_columns'):
            lines.append(f"[SUBSTEP 3.2] One-Hot Encode Protocol")
            lines.append(f"  • Column: Protocol")
            lines.append(f"  • Method: One-hot encoding (creates binary columns)")
            lines.append(f"  • Action: pd.get_dummies(df, columns=['Protocol'], drop_first=False)")
            lines.append(f"  • Before: {enc.get('original_columns', 0)} columns")
            lines.append(f"  • Created columns:")
            for col in enc['protocol_columns']:
                lines.append(f"      {col}")
            lines.append(f"  • After: {enc.get('encoded_columns', 0)} columns (+{enc.get('columns_added', 0)})")
            lines.append(f"  • Validation: Original Protocol column removed ✓")
            lines.append("")
        
        lines.append(f"[SUBSTEP 3.3] Label Encode Target Variable")
        lines.append(f"  • Column: Label")
        lines.append(f"  • Method: Label encoding (string → integer)")
        lines.append(f"  • Action: LabelEncoder().fit_transform(df['Label'])")
        lines.append(f"  • Classes: {enc.get('n_classes', 0)}")
        if 'class_mapping' in enc:
            lines.append(f"  • Mapping:")
            for idx, label in enc['class_mapping'].items():
                lines.append(f"      {idx} ← {label}")
        lines.append(f"  • Validation: All classes encoded ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 3.4] Verify No Categorical Columns Remain")
        lines.append(f"  • Check: df.select_dtypes(include='object').columns")
        lines.append(f"  • Result: No object columns remaining ✓")
        lines.append(f"  • Status: Encoding complete, proceeding to train-test split")
        lines.append("")
        step_num += 1
    
    # STEP 4: TRAIN-TEST SPLIT
    if 'split' in all_stats:
        split = all_stats['split']
        lines.append(f"STEP {step_num}: TRAIN-TEST SPLIT")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Split dataset for training and testing (maintain class proportions)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 4.1] Separate Features and Labels")
        lines.append(f"  • Features (X): All columns except Label")
        lines.append(f"  • Labels (y): Label column")
        lines.append(f"  • Feature count: {split.get('n_features', 0)}")
        lines.append(f"  • Total samples: {format_number(split.get('total_samples', 0))}")
        lines.append("")
        
        lines.append(f"[SUBSTEP 4.2] Configure Split Parameters")
        lines.append(f"  • Test size: {split.get('test_percentage', 0):.0f}% ({split.get('test_size', 0)})")
        lines.append(f"  • Stratify: {'Yes' if split.get('stratified', True) else 'No'} (maintain class proportions)")
        lines.append(f"  • Random state: {split.get('random_state', 42)} (reproducibility)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 4.3] Perform Stratified Split")
        lines.append(f"  • Action: train_test_split(X, y, test_size={split.get('test_size', 0)}, stratify=y, random_state={split.get('random_state', 42)})")
        lines.append(f"  • Training set: {format_number(split.get('n_train', 0))} samples ({split.get('train_percentage', 0):.1f}%)")
        lines.append(f"  • Test set: {format_number(split.get('n_test', 0))} samples ({split.get('test_percentage', 0):.1f}%)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 4.4] Verify Stratification")
        lines.append(f"  • Method: Compare class distributions in train vs test")
        lines.append(f"  • Metric: Max absolute difference in class proportions")
        lines.append(f"  • Result: {split.get('max_distribution_diff', 0)*100:.3f}% max difference")
        lines.append(f"  • Threshold: <1% (excellent)")
        lines.append(f"  • Status: {'✓ VERIFIED' if split.get('max_distribution_diff', 0) < 0.01 else '⚠ WARNING'}")
        lines.append(f"  • Conclusion: Train and test have same class proportions")
        lines.append("")
        step_num += 1
    
    # STEP 5: FEATURE SCALING
    if 'scaling' in all_stats:
        scale = all_stats['scaling']
        lines.append(f"STEP {step_num}: FEATURE SCALING")
        lines.append("=" * 80)
        lines.append("")
        scaler_name = scale.get('scaler_type', 'standard').title() + 'Scaler'
        lines.append(f"Purpose: Normalize features using {scaler_name} (mean=0, std=1)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 5.1] Select Scaler")
        lines.append(f"  • Scaler: {scaler_name}")
        if scale.get('scaler_type') == 'standard':
            lines.append(f"  • Formula: (x - mean) / std")
            lines.append(f"  • Result: mean ≈ 0, std ≈ 1")
        else:
            lines.append(f"  • Formula: (x - min) / (max - min)")
            lines.append(f"  • Result: range [0, 1]")
        lines.append(f"  • Configuration: config.SCALER_TYPE = '{scale.get('scaler_type', 'standard')}'")
        lines.append("")
        
        lines.append(f"[SUBSTEP 5.2] Fit Scaler on Training Data ONLY")
        lines.append(f"  • Action: scaler.fit(X_train)")
        lines.append(f"  • Training samples: {format_number(scale.get('train_shape', (0,))[0])}")
        lines.append(f"  • Features: {scale.get('n_features', 0)}")
        lines.append(f"  • CRITICAL: Scaler learns statistics from TRAINING data ONLY")
        lines.append(f"  • Purpose: Prevent data leakage (test data must not influence scaler)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 5.3] Transform Training Data")
        lines.append(f"  • Action: X_train_scaled = scaler.transform(X_train)")
        lines.append(f"  • Method: Apply learned mean/std from Step 5.2")
        lines.append(f"  • Shape: {scale.get('train_shape', (0, 0))}")
        lines.append(f"  • Validation: Mean ≈ 0, Std ≈ 1 ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 5.4] Transform Test Data")
        lines.append(f"  • Action: X_test_scaled = scaler.transform(X_test)")
        lines.append(f"  • Method: Apply TRAINING statistics (not test statistics)")
        lines.append(f"  • Shape: {scale.get('test_shape', (0, 0))}")
        lines.append(f"  • CRITICAL: Test data did NOT influence scaler")
        lines.append(f"  • Result: No data leakage ✓")
        lines.append("")
        
        lines.append(f"[SUBSTEP 5.5] Save Scaler Object")
        lines.append(f"  • Action: joblib.dump(scaler, 'scaler.joblib')")
        lines.append(f"  • Purpose: Reuse for new data in production")
        lines.append(f"  • Checkpoint: Scaler saved successfully ✓")
        lines.append("")
        step_num += 1
    
    # STEP 6: SMOTE
    if 'smote' in all_stats and all_stats['smote'].get('applied', True):
        smote = all_stats['smote']
        lines.append(f"STEP {step_num}: CLASS IMBALANCE HANDLING (SMOTE)")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Balance minority classes using synthetic oversampling")
        lines.append("")
        
        lines.append(f"[SUBSTEP 6.1] Analyze Class Imbalance")
        lines.append(f"  • Training samples: {format_number(smote.get('before_count', 0))}")
        lines.append(f"  • Classes: {len(smote.get('distribution_before', {}))}")
        if 'distribution_before' in smote:
            lines.append(f"  • Distribution:")
            for cls, count in sorted(smote['distribution_before'].items()):
                pct = count / smote['before_count'] * 100
                lines.append(f"      Class {cls}: {format_number(count):>10s} ({pct:>5.2f}%)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 6.2] Configure SMOTE Parameters")
        smote_strategy = smote.get('strategy', 'dynamic')
        if smote_strategy == 'dynamic':
            lines.append(f"  • Strategy: DYNAMIC (bring minorities halfway to 2nd largest class)")
            lines.append(f"  • Formula: target_count = current + (2nd_largest - current) / 2")
        elif smote_strategy == 'tiered':
            lines.append(f"  • Strategy: TIERED (config-based target percentages per class)")
        else:
            lines.append(f"  • Strategy: UNIFORM (bring minorities to {smote.get('target_percentage', 0)*100:.0f}% of dataset)")
        lines.append(f"  • k_neighbors: {smote.get('k_neighbors', 5)}")
        lines.append(f"  • Random state: {config.RANDOM_STATE}")
        lines.append(f"  • Classes to oversample: {smote.get('classes_oversampled', 0)}")
        lines.append(f"  • CRITICAL: Apply to TRAINING data ONLY (test remains imbalanced)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 6.3] Apply SMOTE")
        lines.append(f"  • Action: X_train_smoted, y_train_smoted = SMOTE(...).fit_resample(X_train, y_train)")
        lines.append(f"  • Before: {format_number(smote.get('before_count', 0))} samples")
        lines.append(f"  • After: {format_number(smote.get('after_count', 0))} samples")
        lines.append(f"  • Synthetic samples: {format_number(smote.get('synthetic_samples', 0))}")
        lines.append(f"  • Synthetic samples generated: {format_number(smote.get('synthetic_samples', 0))}")
        lines.append("")
        
        lines.append(f"[SUBSTEP 6.4] Verify SMOTE Results")
        if 'distribution_after' in smote:
            lines.append(f"  • Final distribution:")
            for cls, count in sorted(smote['distribution_after'].items()):
                pct = count / smote['after_count'] * 100
                before_count = smote['distribution_before'].get(cls, count)
                increase = count - before_count
                factor = count / before_count if before_count > 0 else 0
                if increase > 0:
                    lines.append(f"      Class {cls}: {format_number(count):>10s} ({pct:>5.2f}%) [+{format_number(increase)}, {factor:.1f}x]")
                else:
                    lines.append(f"      Class {cls}: {format_number(count):>10s} ({pct:>5.2f}%)")
        lines.append(f"  • Validation: Minority classes increased ✓")
        lines.append(f"  • Note: Intermediate checkpoint not saved (only final outputs)")
        lines.append("")
        step_num += 1
    
    # STEP 7: FEATURE SELECTION (RF GINI IMPORTANCE)
    if 'rfe' in all_stats and all_stats['rfe'].get('applied', True):
        rfe = all_stats['rfe']
        lines.append(f"STEP {step_num}: FEATURE SELECTION (RANDOM FOREST GINI IMPORTANCE)")
        lines.append("=" * 80)
        lines.append("")
        lines.append("Purpose: Select optimal features using Random Forest feature importance")
        lines.append("")
        
        lines.append(f"[SUBSTEP 7.1] Configure Random Forest Feature Importance")
        lines.append(f"  • Method: Random Forest Gini Importance (fast & effective)")
        lines.append(f"  • Estimator: Random Forest")
        lines.append(f"    - n_estimators: {config.RF_IMPORTANCE_TREES}")
        lines.append(f"    - max_depth: {config.RF_IMPORTANCE_MAX_DEPTH}")
        lines.append(f"    - n_jobs: {config.N_JOBS}")
        use_subset = rfe.get('use_subset', False)
        subset_note = f"Stratified subset ({config.RF_IMPORTANCE_SUBSET_SIZE:,} samples)" if use_subset else f"Full training data ({rfe.get('n_train_samples', 0):,} samples)"
        lines.append(f"  • Dataset: {subset_note}")
        lines.append(f"  • Min features: {config.TARGET_FEATURES_MIN}")
        lines.append(f"  • Target range: {config.TARGET_FEATURES_MIN}-{config.TARGET_FEATURES_MAX} (moderate)")
        lines.append(f"  • Importance threshold: {rfe.get('importance_threshold', 0.0001):.6f}")
        lines.append("")
        
        lines.append(f"[SUBSTEP 7.2] Train Random Forest & Extract Importances")
        lines.append(f"  • Action: rf.fit(X_subset, y_subset) | importances = rf.feature_importances_")
        lines.append(f"  • Initial features: {rfe.get('n_features_before', 0)}")
        lines.append(f"  • Features evaluated: {rfe.get('n_features_before', 0)}")
        lines.append(f"  • Method: Rank features by Gini importance, select top N")
        lines.append("")
        
        lines.append(f"[SUBSTEP 7.3] Analyze Feature Importance Results")
        lines.append(f"  • Selected features: {rfe.get('n_features_selected', 0)}")
        lines.append(f"  • Features above threshold: {rfe.get('features_above_threshold', rfe.get('n_features_selected', 0))}")
        lines.append(f"  • Reduction: {rfe.get('reduction_percentage', 0):.1f}%")
        lines.append(f"  • Status: ✓ Feature selection complete")
        lines.append("")
        
        lines.append(f"[SUBSTEP 7.4] Apply Feature Selection")
        lines.append(f"  • Action: X_train_rfe = X_train[selected_features]")
        lines.append(f"  • Action: X_test_rfe = X_test[selected_features]")
        lines.append(f"  • Training shape: {rfe.get('n_features_selected', 0)} features")
        lines.append(f"  • Test shape: {rfe.get('n_features_selected', 0)} features (same)")
        lines.append("")
        
        lines.append(f"[SUBSTEP 7.5] Save Selected Features")
        if 'selected_features' in rfe:
            lines.append(f"  • Selected features ({len(rfe['selected_features'])}):")
            selected_features = rfe['selected_features']
            for i, feat in enumerate(selected_features, 1):
                lines.append(f"      {i:2d}. {feat}")
        lines.append(f"  • Artifacts: selected_features.joblib, feature_importances.csv")
        lines.append(f"  • Model: rf_importance_model.joblib")
        lines.append("")
        step_num += 1
    
    # FINAL SUMMARY
    lines.append("=" * 80)
    lines.append(" " * 22 + "PREPROCESSING COMPLETED SUCCESSFULLY")
    lines.append("=" * 80)
    lines.append("")
    lines.append(f"Total Steps: {step_num - 1}")
    lines.append(f"Artifacts Saved:")
    lines.append(f"  • scaler.joblib, label_encoder.joblib, selected_features.joblib")
    lines.append(f"  • feature_importances.csv, rf_importance_model.joblib")
    lines.append(f"Models Saved: {2 if 'rfe' in all_stats and all_stats['rfe'].get('applied', True) else 1}")
    lines.append(f"Reports Generated: 2")
    lines.append("")
    lines.append(f"Final Dataset Ready for Training:")
    if 'split' in all_stats:
        lines.append(f"  • Training: {format_number(all_stats.get('smote', {}).get('after_count', all_stats['split'].get('n_train', 0)))} samples")
        lines.append(f"  • Test: {format_number(all_stats['split'].get('n_test', 0))} samples")
        if 'rfe' in all_stats and all_stats['rfe'].get('applied', True):
            lines.append(f"  • Features: {all_stats['rfe'].get('n_features_selected', 0)}")
        else:
            lines.append(f"  • Features: {all_stats['split'].get('n_features', 0)}")
        lines.append(f"  • Classes: {all_stats.get('encoding', {}).get('n_classes', 8)}")
    lines.append("")
    lines.append("=" * 80)
    lines.append(f"Next Module: Model Training (Module 4)")
    lines.append("=" * 80)
    lines.append("")
    
    steps_path = os.path.join(output_dir, 'preprocessing_steps.txt')
    write_text_report('\n'.join(lines), steps_path)
    log_message(f"✓ Saved detailed step-by-step log: preprocessing_steps.txt", level="SUCCESS")


if __name__ == "__main__":
    # Test the module
    from ml_model.data_loader import load_data
    
    df, label_col, protocol_col, _ = load_data()
    result = preprocess_data(df, label_col, protocol_col)
    print("\nPreprocessing completed!")
